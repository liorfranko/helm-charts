# You can be override only a whole group
PrometheusRules:
  core-dns-record-rules:
    groups:
    - name: core-dns-record-rules
      rules:
        - record: 'coredns:cache_hits_rate'
          expr: |
            sum by (kubernetes_cluster)((delta(coredns_cache_hits_total{instance!~"node-local-.*"}[1m])))
        - record: 'coredns:cache_hits_rate_zscore'
          expr: |
            (coredns:cache_hits_rate - avg_over_time(coredns:cache_hits_rate[1d:10m]) ) / (stddev_over_time(coredns:cache_hits_rate[1d:10m]))
        - record: 'coredns:requests_rate'
          expr: |
            sum(delta(coredns_dns_request_type_count_total{instance!~"node-local-.*"}[1m])) by (type, kubernetes_cluster)
        - record: 'coredns:requests_rate_zscore'
          expr: |
            (coredns:requests_rate - avg_over_time(coredns:requests_rate[1d:10m]) ) / (stddev_over_time(coredns:requests_rate[1d:10m]))
        - record: 'coredns:error_rate'
          expr: |
            sum(delta(coredns_dns_response_rcode_count_total{instance!~"node-local-.*"}[1m])) by(zone,rcode, kubernetes_cluster)
        - record: 'coredns:error_rate_zscore'
          expr: |
            (coredns:error_rate - avg_over_time(coredns:error_rate[1d:10m]) ) / (stddev_over_time(coredns:error_rate[1d:10m]))
        - record: 'coredns:requests_rate_zscore_abs'
          expr: |
            abs(coredns:cache_requests_rate_zscore)
        - record: 'coredns:error_rate_zscore_abs'
          expr: |
            abs(coredns:cache_error_rate_zscore)
        - record: 'coredns:cache_hits_rate_zscore_abs'
          expr: |
            abs(coredns:cache_hits_rate_zscore)
  k8s-record-rules:
    groups:
    - name: k8s-record-rules
      rules:
      # Node record rules
      - record: kube_node:node_boot_time_seconds_per_instance_name:sum
        expr: |
          sum without (private_ip, public_ip, instance, label_kubernetes_io_instance_type)(node_boot_time_seconds)

      - record: 'node:node_status_pods_capacity:'
        expr: |
          label_replace(kube_node_status_capacity_pods{job=~"kubernetes_.+"},"instance_name", "$1", "node", "(.*)")

      - record: 'node:node_running_pod_count:sum'
        expr: |
          label_replace(kubelet_running_pod_count{job=~"kubernetes_.+"}, "instance_name", "$1", "instance", "(.*)")

      - record: 'node:cpu:sum'
        expr: |
          count by (kubernetes_cluster, instance_name) (node_cpu_seconds_total{job=~"kubernetes_.+", mode="system"})

      - record: 'node:node_allocatable_cpu_cores:sum'
        expr: |
          label_replace(kube_node_status_allocatable_cpu_cores{job=~"kubernetes_.+"}, "instance_name", "$1", "instance", "(.*)")

      - record: 'node:node_container_cpu_usage_seconds:irate1m'
        expr: |
          sum by(kubernetes_cluster, instance_name)(label_replace(irate(container_cpu_usage_seconds_total{job=~"kubernetes_.+",  container!="POD", container!=""}[1m]), "instance_name", "$1", "instance", "(.*)"))

      - record: 'node:node_allocatable_memory_bytes:sum'
        expr: |
          label_replace(kube_node_status_allocatable_memory_bytes{job=~"kubernetes_.+"}, "instance_name", "$1", "node", "(.*)")

      - record: 'node:node_container_memory_wss_bytes:sum'
        expr: |
          sum by(kubernetes_cluster, instance_name)(label_replace(container_memory_working_set_bytes{job=~"kubernetes_.+",  container!="POD", container!=""}, "instance_name", "$1", "instance", "(.*)"))

      - record: 'node:node_filesystem_usage:ratio'
        expr: |
          max by (kubernetes_cluster, instance_name, mountpoint) ((node_filesystem_size_bytes{job=~"kubernetes_.+", fstype=~"ext[234]|btrfs|xfs|zfs"} - node_filesystem_avail_bytes{job=~"kubernetes_.+", fstype=~"ext[234]|btrfs|xfs|zfs"}) /  node_filesystem_size_bytes{job=~"kubernetes_.+", fstype=~"ext[234]|btrfs|xfs|zfs"})

      - record: 'node:node_filesystem_avail:ratio'
        expr: |
          max by (kubernetes_cluster, instance_name, mountpoint) (node_filesystem_avail_bytes{job=~"kubernetes_.+", fstype=~"ext[234]|btrfs|xfs|zfs"} / node_filesystem_size_bytes{job=~"kubernetes_.+", fstype=~"ext[234]|btrfs|xfs|zfs"})

      - record: 'kubelet:persistent_volume_avail:ratio'
        expr: |
          kubelet_volume_stats_available_bytes{job=~"kubernetes_.+"} / kubelet_volume_stats_capacity_bytes{job=~"kubernetes_.+"}

      - record: 'kubelet:persistent_volume_usage:ratio'
        expr: |
          kubelet_volume_stats_used_bytes{job=~"kubernetes_.+"} / kubelet_volume_stats_capacity_bytes{job=~"kubernetes_.+"}

      - record: 'kubelet:persistent_volume_inode_avail:ratio'
        expr: |
          kubelet_volume_stats_inodes_free{job=~"kubernetes_.+"} / kubelet_volume_stats_inodes{job=~"kubernetes_.+"}

      - record: 'kubelet:persistent_volume_inode_usage:ratio'
        expr: |
          kubelet_volume_stats_inodes_used{job=~"kubernetes_.+"} / kubelet_volume_stats_inodes{job=~"kubernetes_.+"}

      # Namespace record rules
      - record: 'namespace:kube_pod_container_resource_requests_cpu_cores:sum'
        expr: |
          sum by(kubernetes_cluster, namespace, pod)(kube_pod_container_resource_requests_cpu_cores{job=~"kubernetes_.+"} * on (kubernetes_cluster, job, instance, namespace, pod) group_left(phase) (kube_pod_status_phase{job=~"kubernetes_.+", phase=~"^(Pending|Running)$"} == 1))

      - record: 'namespace:kube_pod_container_resource_requests_memory_bytes:sum'
        expr: |
          sum by(kubernetes_cluster, namespace, pod)(kube_pod_container_resource_requests_memory_bytes{job=~"kubernetes_.+"} * on (kubernetes_cluster, job, instance, namespace, pod) group_left(phase) (kube_pod_status_phase{job=~"kubernetes_.+", phase=~"^(Pending|Running)$"} == 1))

      # Cest/cet time records
      # https://medium.com/@tom.fawcett/time-of-day-based-notifications-with-prometheus-and-alertmanager-1bf7a23b7695
      - record: is_israel_summer_time
        expr: |
          (vector(1) and (month() > 3 and month() < 10))
          or
          (vector(1) and (month() == 3 and (day_of_month() - day_of_week()) >= 25) and absent((day_of_month() >= 25) and (day_of_week() == 0)))
          or
          (vector(1) and (month() == 10 and (day_of_month() - day_of_week()) < 25) and absent((day_of_month() >= 25) and (day_of_week() == 0)))
          or
          (vector(1) and ((month() == 10 and hour() < 1) or (month() == 3 and hour() > 0)) and ((day_of_month() >= 25) and (day_of_week() == 0)))
          or
          vector(0)
      - record: israel_localtime
        expr: time() + 3600 + 3600 + 3600 * is_israel_summer_time
      - record: business_day
        expr: vector(1) and day_of_week(israel_localtime) >= 0 and day_of_week(israel_localtime) < 5
      - record: israel_hour
        expr: hour(israel_localtime)
      - record: business_hour
        expr: vector(1) and israel_hour >= 9 < 18 and business_day

      # - record: europe_london_time
      #   expr: time() + 3600 * is_european_summer_time
      # - record: europe_london_hour
      #   expr: hour(europe_london_time)
      # - record: central_europe_time
      #   expr: time() + 3600 + 3600 * is_european_summer_time
      # - record: central_europe_hour
      #   expr: hour(central_europe_time)
      - record: kube_cronjob_last_execution
        expr: |
          label_replace(
            label_replace(
              max(
                max by (job_name,kubernetes_cluster, namespace)(kube_job_status_start_time)
                * ON(job_name,kubernetes_cluster,namespace) GROUP_RIGHT()
                kube_job_labels{label_cron!=""}
              ) BY (job_name, label_cron,kubernetes_cluster,namespace)
              == ON(label_cron,kubernetes_cluster,namespace) GROUP_LEFT()
              max(
                max by (job_name, kubernetes_cluster,namespace)(kube_job_status_start_time)
                * ON(job_name, kubernetes_cluster,namespace) GROUP_RIGHT()
                kube_job_labels{label_cron!=""}
              ) BY (label_cron, kubernetes_cluster,namespace),
              "job", "$1", "job_name", "(.+)"),
            "cronjob", "$1", "label_cron", "(.+)")
      - record: kube_cronjob_last_failed
        expr: |
          clamp_max(
            kube_cronjob_last_execution,
          1)
          * ON(job_name, kubernetes_cluster, namespace) GROUP_LEFT()
          label_replace(
            label_replace(
              (kube_job_status_failed != 0),
              "job", "$1", "job_name", "(.+)"),
            "cronjob", "$1", "label_cron", "(.+)")
      - expr: sum(rate(container_cpu_usage_seconds_total{job="kubelet", metrics_path="/metrics/cadvisor", image!="", container!="POD"}[5m])) by (namespace)
        record: namespace:container_cpu_usage_seconds_total:sum_rate
      - expr: |-
          sum by (cluster, namespace, pod, container) (
            rate(container_cpu_usage_seconds_total{job="kubelet", metrics_path="/metrics/cadvisor", image!="", container!="POD"}[5m])
          ) * on (cluster, namespace, pod) group_left(node) topk by (cluster, namespace, pod) (
            1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
      - expr: |-
          container_memory_working_set_bytes{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
          * on (namespace, pod) group_left(node) topk by(namespace, pod) (1,
            max by(namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_memory_working_set_bytes
      - expr: |-
          container_memory_rss{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
          * on (namespace, pod) group_left(node) topk by(namespace, pod) (1,
            max by(namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_memory_rss
      - expr: |-
          container_memory_cache{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
          * on (namespace, pod) group_left(node) topk by(namespace, pod) (1,
            max by(namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_memory_cache
      - expr: |-
          container_memory_swap{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
          * on (namespace, pod) group_left(node) topk by(namespace, pod) (1,
            max by(namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_memory_swap
      - expr: sum(container_memory_usage_bytes{job="kubelet", metrics_path="/metrics/cadvisor", image!="", container!="POD"}) by (namespace)
        record: namespace:container_memory_usage_bytes:sum
      - expr: |-
          sum by (namespace) (
              sum by (namespace, pod) (
                  max by (namespace, pod, container) (
                      kube_pod_container_resource_requests_memory_bytes{job="kube-state-metrics"}
                  ) * on(namespace, pod) group_left() max by (namespace, pod) (
                      kube_pod_status_phase{phase=~"Pending|Running"} == 1
                  )
              )
          )
        record: namespace:kube_pod_container_resource_requests_memory_bytes:sum
      - expr: |-
          sum by (namespace) (
              sum by (namespace, pod) (
                  max by (namespace, pod, container) (
                      kube_pod_container_resource_requests_cpu_cores{job="kube-state-metrics"}
                  ) * on(namespace, pod) group_left() max by (namespace, pod) (
                    kube_pod_status_phase{phase=~"Pending|Running"} == 1
                  )
              )
          )
        record: namespace:kube_pod_container_resource_requests_cpu_cores:sum
      - expr: |-
          max by (cluster, namespace, workload, pod) (
            label_replace(
              label_replace(
                kube_pod_owner{job="kube-state-metrics", owner_kind="ReplicaSet"},
                "replicaset", "$1", "owner_name", "(.*)"
              ) * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (
                1, max by (replicaset, namespace, owner_name) (
                  kube_replicaset_owner{job="kube-state-metrics"}
                )
              ),
              "workload", "$1", "owner_name", "(.*)"
            )
          )
        labels:
          workload_type: deployment
        record: namespace_workload_pod:kube_pod_owner:relabel
      - expr: |-
          max by (cluster, namespace, workload, pod) (
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="DaemonSet"},
              "workload", "$1", "owner_name", "(.*)"
            )
          )
        labels:
          workload_type: daemonset
        record: namespace_workload_pod:kube_pod_owner:relabel
      - expr: |-
          max by (cluster, namespace, workload, pod) (
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="StatefulSet"},
              "workload", "$1", "owner_name", "(.*)"
            )
          )
        labels:
          workload_type: statefulset
        record: namespace_workload_pod:kube_pod_owner:relabel
  kube-apiserver-availability-record-rules:
    groups:
    - interval: 3m
      name: kube-apiserver-availability-record-rules
      rules:
      - expr: |-
          1 - (
            (
              # write too slow
              sum(increase(apiserver_request_duration_seconds_count{verb=~"POST|PUT|PATCH|DELETE"}[30d]))
              -
              sum(increase(apiserver_request_duration_seconds_bucket{verb=~"POST|PUT|PATCH|DELETE",le="1"}[30d]))
            ) +
            (
              # read too slow
              sum(increase(apiserver_request_duration_seconds_count{verb=~"LIST|GET"}[30d]))
              -
              (
                (
                  sum(increase(apiserver_request_duration_seconds_bucket{verb=~"LIST|GET",scope=~"resource|",le="0.1"}[30d]))
                  or
                  vector(0)
                )
                +
                sum(increase(apiserver_request_duration_seconds_bucket{verb=~"LIST|GET",scope="namespace",le="0.5"}[30d]))
                +
                sum(increase(apiserver_request_duration_seconds_bucket{verb=~"LIST|GET",scope="cluster",le="5"}[30d]))
              )
            ) +
            # errors
            sum(code:apiserver_request_total:increase30d{code=~"5.."} or vector(0))
          )
          /
          sum(code:apiserver_request_total:increase30d)
        labels:
          verb: all
        record: apiserver_request:availability30d
      - expr: |-
          1 - (
            sum(increase(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[30d]))
            -
            (
              # too slow
              (
                sum(increase(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[30d]))
                or
                vector(0)
              )
              +
              sum(increase(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[30d]))
              +
              sum(increase(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[30d]))
            )
            +
            # errors
            sum(code:apiserver_request_total:increase30d{verb="read",code=~"5.."} or vector(0))
          )
          /
          sum(code:apiserver_request_total:increase30d{verb="read"})
        labels:
          verb: read
        record: apiserver_request:availability30d
      - expr: |-
          1 - (
            (
              # too slow
              sum(increase(apiserver_request_duration_seconds_count{verb=~"POST|PUT|PATCH|DELETE"}[30d]))
              -
              sum(increase(apiserver_request_duration_seconds_bucket{verb=~"POST|PUT|PATCH|DELETE",le="1"}[30d]))
            )
            +
            # errors
            sum(code:apiserver_request_total:increase30d{verb="write",code=~"5.."} or vector(0))
          )
          /
          sum(code:apiserver_request_total:increase30d{verb="write"})
        labels:
          verb: write
        record: apiserver_request:availability30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="LIST",code=~"2.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="GET",code=~"2.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="POST",code=~"2.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PUT",code=~"2.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PATCH",code=~"2.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="DELETE",code=~"2.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="LIST",code=~"3.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="GET",code=~"3.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="POST",code=~"3.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PUT",code=~"3.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PATCH",code=~"3.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="DELETE",code=~"3.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="LIST",code=~"4.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="GET",code=~"4.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="POST",code=~"4.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PUT",code=~"4.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PATCH",code=~"4.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="DELETE",code=~"4.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="LIST",code=~"5.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="GET",code=~"5.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="POST",code=~"5.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PUT",code=~"5.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PATCH",code=~"5.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="DELETE",code=~"5.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code) (code_verb:apiserver_request_total:increase30d{verb=~"LIST|GET"})
        labels:
          verb: read
        record: code:apiserver_request_total:increase30d
      - expr: sum by (code) (code_verb:apiserver_request_total:increase30d{verb=~"POST|PUT|PATCH|DELETE"})
        labels:
          verb: write
        record: code:apiserver_request_total:increase30d
  kube-apiserver-record-rules:
    groups:
    - name: kube-apiserver-record-rules
      rules:
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[1d]))
              -
              (
                (
                  sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[1d]))
                  or
                  vector(0)
                )
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[1d]))
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[1d]))
              )
            )
            +
            # errors
            sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[1d]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1d]))
        labels:
          verb: read
        record: apiserver_request:burnrate1d
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[1h]))
              -
              (
                (
                  sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[1h]))
                  or
                  vector(0)
                )
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[1h]))
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[1h]))
              )
            )
            +
            # errors
            sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[1h]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1h]))
        labels:
          verb: read
        record: apiserver_request:burnrate1h
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[2h]))
              -
              (
                (
                  sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[2h]))
                  or
                  vector(0)
                )
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[2h]))
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[2h]))
              )
            )
            +
            # errors
            sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[2h]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[2h]))
        labels:
          verb: read
        record: apiserver_request:burnrate2h
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[30m]))
              -
              (
                (
                  sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[30m]))
                  or
                  vector(0)
                )
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[30m]))
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[30m]))
              )
            )
            +
            # errors
            sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[30m]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[30m]))
        labels:
          verb: read
        record: apiserver_request:burnrate30m
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[3d]))
              -
              (
                (
                  sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[3d]))
                  or
                  vector(0)
                )
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[3d]))
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[3d]))
              )
            )
            +
            # errors
            sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[3d]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[3d]))
        labels:
          verb: read
        record: apiserver_request:burnrate3d
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[5m]))
              -
              (
                (
                  sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[5m]))
                  or
                  vector(0)
                )
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[5m]))
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[5m]))
              )
            )
            +
            # errors
            sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[5m]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[5m]))
        labels:
          verb: read
        record: apiserver_request:burnrate5m
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[6h]))
              -
              (
                (
                  sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[6h]))
                  or
                  vector(0)
                )
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[6h]))
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[6h]))
              )
            )
            +
            # errors
            sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[6h]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[6h]))
        labels:
          verb: read
        record: apiserver_request:burnrate6h
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1d]))
              -
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[1d]))
            )
            +
            sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[1d]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1d]))
        labels:
          verb: write
        record: apiserver_request:burnrate1d
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1h]))
              -
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[1h]))
            )
            +
            sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[1h]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1h]))
        labels:
          verb: write
        record: apiserver_request:burnrate1h
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[2h]))
              -
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[2h]))
            )
            +
            sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[2h]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[2h]))
        labels:
          verb: write
        record: apiserver_request:burnrate2h
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[30m]))
              -
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[30m]))
            )
            +
            sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[30m]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[30m]))
        labels:
          verb: write
        record: apiserver_request:burnrate30m
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[3d]))
              -
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[3d]))
            )
            +
            sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[3d]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[3d]))
        labels:
          verb: write
        record: apiserver_request:burnrate3d
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
              -
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[5m]))
            )
            +
            sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[5m]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
        labels:
          verb: write
        record: apiserver_request:burnrate5m
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[6h]))
              -
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[6h]))
            )
            +
            sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[6h]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[6h]))
        labels:
          verb: write
        record: apiserver_request:burnrate6h
      - expr: sum by (code,resource) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[5m]))
        labels:
          verb: read
        record: code_resource:apiserver_request_total:rate5m
      - expr: sum by (code,resource) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
        labels:
          verb: write
        record: code_resource:apiserver_request_total:rate5m
      - expr: histogram_quantile(0.99, sum by (le, resource) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET"}[5m]))) > 0
        labels:
          quantile: '0.99'
          verb: read
        record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.99, sum by (le, resource) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))) > 0
        labels:
          quantile: '0.99'
          verb: write
        record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
      - expr: |-
          sum(rate(apiserver_request_duration_seconds_sum{subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod)
          /
          sum(rate(apiserver_request_duration_seconds_count{subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod)
        record: cluster:apiserver_request_duration_seconds:mean5m
      - expr: histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod))
        labels:
          quantile: '0.99'
        record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.9, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod))
        labels:
          quantile: '0.9'
        record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.5, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod))
        labels:
          quantile: '0.5'
        record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
  kubelet-record-rules:
    groups:
    - name: kubelet-record-rules
      rules:
      - expr: histogram_quantile(0.99, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (instance, le) * on(instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
        labels:
          quantile: '0.99'
        record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.9, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (instance, le) * on(instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
        labels:
          quantile: '0.9'
        record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.5, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (instance, le) * on(instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
        labels:
          quantile: '0.5'
        record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
  kube-prometheus-general-record-rules:
    groups:
    - name: kube-prometheus-general-record-rules
      rules:
      - expr: count without(instance, pod, node) (up == 1)
        record: count:up1
      - expr: count without(instance, pod, node) (up == 0)
        record: count:up0
  kube-prometheus-node-record-rules:
    groups:
    - name: kube-prometheus-node-record-rules
      rules:
      - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[3m])) BY (instance)
        record: instance:node_cpu:rate:sum
      - expr: sum(rate(node_network_receive_bytes_total[3m])) BY (instance)
        record: instance:node_network_receive_bytes:rate:sum
      - expr: sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)
        record: instance:node_network_transmit_bytes:rate:sum
      - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[5m])) WITHOUT (cpu, mode) / ON(instance) GROUP_LEFT() count(sum(node_cpu_seconds_total) BY (instance, cpu)) BY (instance)
        record: instance:node_cpu:ratio
      - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[5m]))
        record: cluster:node_cpu:sum_rate5m
      - expr: cluster:node_cpu_seconds_total:rate5m / count(sum(node_cpu_seconds_total) BY (instance, cpu))
        record: cluster:node_cpu:ratio
  kube-scheduler-record-rules:
    groups:
    - name: kube-scheduler-record-rules
      rules:
      - expr: histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: '0.99'
        record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: '0.99'
        record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.99, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: '0.99'
        record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: '0.9'
        record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: '0.9'
        record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.9, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: '0.9'
        record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: '0.5'
        record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: '0.5'
        record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.5, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: '0.5'
        record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
  node-exporter-record-rules:
    groups:
    - name: node-exporter-record-rules
      rules:
      - expr: |-
          count without (cpu) (
            count without (mode) (
              node_cpu_seconds_total{job="node-exporter"}
            )
          )
        record: instance:node_num_cpu:sum
      - expr: |-
          1 - avg without (cpu, mode) (
            rate(node_cpu_seconds_total{job="node-exporter", mode="idle"}[1m])
          )
        record: instance:node_cpu_utilisation:rate1m
      - expr: |-
          (
            node_load1{job="node-exporter"}
          /
            instance:node_num_cpu:sum{job="node-exporter"}
          )
        record: instance:node_load1_per_cpu:ratio
      - expr: |-
          1 - (
            node_memory_MemAvailable_bytes{job="node-exporter"}
          /
            node_memory_MemTotal_bytes{job="node-exporter"}
          )
        record: instance:node_memory_utilisation:ratio
      - expr: rate(node_vmstat_pgmajfault{job="node-exporter"}[1m])
        record: instance:node_vmstat_pgmajfault:rate1m
      - expr: rate(node_disk_io_time_seconds_total{job="node-exporter", device=~"mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
        record: instance_device:node_disk_io_time_seconds:rate1m
      - expr: rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
        record: instance_device:node_disk_io_time_weighted_seconds:rate1m
      - expr: |-
          sum without (device) (
            rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_receive_bytes_excluding_lo:rate1m
      - expr: |-
          sum without (device) (
            rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_transmit_bytes_excluding_lo:rate1m
      - expr: |-
          sum without (device) (
            rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_receive_drop_excluding_lo:rate1m
      - expr: |-
          sum without (device) (
            rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_transmit_drop_excluding_lo:rate1m
  node-record-rules:
    groups:
    - name: node-record-rules
      rules:
      - expr: sum(min(kube_pod_info{node!=""}) by (cluster, node))
        record: ':kube_pod_info_node_count:'
      - expr: |-
          topk by(namespace, pod) (1,
            max by (node, namespace, pod) (
              label_replace(kube_pod_info{job="kube-state-metrics",node!=""}, "pod", "$1", "pod", "(.*)")
          ))
        record: 'node_namespace_pod:kube_pod_info:'
      - expr: |-
          count by (cluster, node) (sum by (node, cpu) (
            node_cpu_seconds_total{job="node-exporter"}
          * on (namespace, pod) group_left(node)
            node_namespace_pod:kube_pod_info:
          ))
        record: node:node_num_cpu:sum
      - expr: |-
          sum(
            node_memory_MemAvailable_bytes{job="node-exporter"} or
            (
              node_memory_Buffers_bytes{job="node-exporter"} +
              node_memory_Cached_bytes{job="node-exporter"} +
              node_memory_MemFree_bytes{job="node-exporter"} +
              node_memory_Slab_bytes{job="node-exporter"}
            )
          ) by (cluster)
        record: :node_memory_MemAvailable_bytes:sum
  kubecost:
    groups:
      - name: CPU
        rules:
          - expr: sum(rate(container_cpu_usage_seconds_total{container_name!=""}[5m]))
            record: cluster:cpu_usage:rate5m
          - expr: rate(container_cpu_usage_seconds_total{container_name!=""}[5m])
            record: cluster:cpu_usage_nosum:rate5m
          - expr: avg(irate(container_cpu_usage_seconds_total{container_name!="POD", container_name!=""}[5m])) by (container_name,pod_name,namespace)
            record: kubecost_container_cpu_usage_irate
          - expr: sum(container_memory_working_set_bytes{container_name!="POD",container_name!=""}) by (container_name,pod_name,namespace)
            record: kubecost_container_memory_working_set_bytes
          - expr: sum(container_memory_working_set_bytes{container_name!="POD",container_name!=""})
            record: kubecost_cluster_memory_working_set_bytes
      - name: Savings
        rules:
          - expr: sum(avg(kube_pod_owner{owner_kind!="DaemonSet"}) by (pod) * sum(container_cpu_allocation) by (pod))
            record: kubecost_savings_cpu_allocation
            labels:
              daemonset: "false"
          - expr: sum(avg(kube_pod_owner{owner_kind="DaemonSet"}) by (pod) * sum(container_cpu_allocation) by (pod)) / sum(kube_node_info)
            record: kubecost_savings_cpu_allocation
            labels:
              daemonset: "true"
          - expr: sum(avg(kube_pod_owner{owner_kind!="DaemonSet"}) by (pod) * sum(container_memory_allocation_bytes) by (pod))
            record: kubecost_savings_memory_allocation_bytes
            labels:
              daemonset: "false"
          - expr: sum(avg(kube_pod_owner{owner_kind="DaemonSet"}) by (pod) * sum(container_memory_allocation_bytes) by (pod)) / sum(kube_node_info)
            record: kubecost_savings_memory_allocation_bytes
            labels:
              daemonset: "true"
          - expr: label_replace(sum(kube_pod_status_phase{phase="Running",namespace!="kube-system"} > 0) by (pod, namespace), "pod_name", "$1", "pod", "(.+)")
            record: kubecost_savings_running_pods
          - expr: sum(rate(container_cpu_usage_seconds_total{container_name!="",container_name!="POD",instance!=""}[5m])) by (namespace, pod_name, container_name, instance)
            record: kubecost_savings_container_cpu_usage_seconds
          - expr: sum(container_memory_working_set_bytes{container_name!="",container_name!="POD",instance!=""}) by (namespace, pod_name, container_name, instance)
            record: kubecost_savings_container_memory_usage_bytes
          - expr: avg(sum(kube_pod_container_resource_requests_cpu_cores{namespace!="kube-system"}) by (pod, namespace, instance)) by (pod, namespace)
            record: kubecost_savings_pod_requests_cpu_cores
          - expr: avg(sum(kube_pod_container_resource_requests_memory_bytes{namespace!="kube-system"}) by (pod, namespace, instance)) by (pod, namespace)
            record: kubecost_savings_pod_requests_memory_bytes

PrometheusAlerts:
  argocd-alerts:
    ArgoCDAppOutOfSync:
      expr: argocd_app_info{sync_status="OutOfSync", name!="airflow"} > 0
      annotations:
        message: "ArgoCD Application {{ $labels.name }} is OutOfSync for longer than 10 minutes"
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    ArgoCDAppProgressing:
      expr: argocd_app_info{health_status="Progressing", name!="jaeger"} > 0 #Remove the name!="jaeger" once this solved https://github.com/jaegertracing/helm-charts/issues/207
      annotations:
        message: "ArgoCD Application {{ $labels.name }} Progressing longer than 10 minutes"
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    ArgoCDAppUnknown:
      expr: argocd_app_info{sync_status="Unknown"} > 0
      annotations:
        message: "ArgoCD Application {{ $labels.name }} Unknown for 5 minute"
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    ArgoCDAppMissing:
      expr: argocd_app_info{health_status="Missing"} > 0
      annotations:
        message: "ArgoCD Application {{ $labels.name }} Missing for 10 minutes"
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    ArgoCDAppDegraded:
      expr: argocd_app_info{health_status="Degraded"} > 0
      annotations:
        message: "ArgoCD Application {{ $labels.name }} Degraded for 10 minutes"
      for: 10m
      labels:
        severity: warning
        controlPlane: true
  istio-alerts:
    IstioPilotAvailabilityDrop:
      annotations:
        summary: 'Istio Pilot Availability Drop'
        description: 'Pilot pods have dropped during the last 5m (current value: *{{ printf "%2.0f%%" $value }}*). Envoy sidecars might have outdated configuration'
      expr: >
        avg(avg_over_time(up{job="pilot"}[1m])) < 0.5
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    IstioMixerTelemetryAvailabilityDrop:
      annotations:
        summary: 'Istio Mixer Telemetry Drop'
        description: 'Mixer pods have dropped during the last 5m (current value: *{{ printf "%2.0f%%" $value }}*). Istio metrics will not work correctly'
      expr: >
        avg(avg_over_time(up{job="mixer", service="istio-telemetry", endpoint="http-monitoring"}[5m])) < 0.5
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    IstioGalleyAvailabilityDrop:
      annotations:
        summary: 'Istio Galley Availability Drop'
        description: 'Galley pods have dropped during the last 5m (current value: *{{ printf "%2.0f%%" $value }}*). Istio config ingestion and processing will not work'
      expr: >
        avg(avg_over_time(up{job="galley"}[5m])) < 0.5
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    IstioApp01GatewayAvailabilityDrop:
      annotations:
        summary: 'Istio Gateway Availability Drop'
        description: 'Gateway pods have dropped during the last 5m (current value: *{{ printf "%2.0f%%" $value }}*). Inbound traffic will likely be affected'
      expr: >
        min(kube_deployment_status_replicas_available{deployment="app01-ingressgateway", namespace="istio-ingress"}) without (instance, pod) < 2
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    IstioInfraGatewayAvailabilityDrop:
      annotations:
        summary: 'Istio Gateway Availability Drop'
        description: 'Gateway pods have dropped during the last 5m (current value: *{{ printf "%2.0f%%" $value }}*). Inbound traffic will likely be affected'
      expr: >
        min(kube_deployment_status_replicas_available{deployment="infra-ingressgateway", namespace="istio-ingress"}) without (instance, pod) < 2
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    IstioPilotPushErrorsHigh:
      annotations:
        summary: 'Number of Istio Pilot push errors is too high'
        description: 'Pilot has too many push errors during the last 5m (current value: *{{ printf "%2.0f%%" $value }}*). Envoy sidecars might have outdated configuration'
      expr: >
        sum(irate(pilot_xds_push_errors{job="pilot"}[5m])) / sum(irate(pilot_xds_pushes{job="pilot"}[5m])) > 0.05
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    IstioMixerPrometheusDispatchesLow:
      annotations:
        summary: 'Number of Mixer dispatches to Prometheus is too low'
        description: 'Mixer disptaches to Prometheus has dropped below normal levels during the last 5m (current value: *{{ printf "%2.0f%%" $value }}*). Istio metrics might not be being exported properly'
      expr: >
        sum(irate(mixer_runtime_dispatches_total{adapter=~"prometheus"}[5m])) < 180
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    IstioGlobalRequestRateHigh:
      annotations:
        summary: 'Istio Global Request Rate High'
        description: 'Istio global request rate is unusually high during the last 5m (current value: *{{ printf "%2.0f%%" $value }}*). The amount of traffic being generated inside the service mesh is higher than normal'
      expr: >
        round(sum(irate(istio_requests_total{reporter="destination"}[5m])), 0.001) > 1200
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    IstioGlobalRequestRateLow:
      annotations:
        summary: 'Istio global request rate too low'
        description: 'Istio global request rate is unusually low during the last 5m (current value: *{{ printf "%2.0f%%" $value }}*). The amount of traffic being generated inside the service mesh has dropped below usual levels'
      expr: >
        round(sum(irate(istio_requests_total{reporter="destination"}[5m])), 0.001) < 300
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    IstioGlobalHTTP5xxRateHigh:
      annotations:
        summary: 'Istio Percentage of HTTP 5xx responses is too high'
        description: 'Istio global HTTP 5xx rate is too high in last 5m (current value: *{{ printf "%2.0f%%" $value }}*). The HTTP 5xx errors within the service mesh is unusually high'
      expr: >
        sum(irate(istio_requests_total{reporter="destination", response_code=~"5.*"}[5m])) / sum(irate(istio_requests_total{reporter="destination"}[5m])) > 0.01
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    IstioGatewayOutgoingSuccessLow:
      annotations:
        summary: 'Istio Gateway outgoing success rate is too low'
        description: 'Istio Gateway success to outbound destinations is too low in last 5m (current value: *{{ printf "%2.0f%%" $value }}*). Inbound traffic may be affected'
      expr: >
        sum(irate(istio_requests_total{reporter="source", source_workload="istio-ingressgateway",source_workload_namespace="istio-system", connection_security_policy!="mutual_tls",response_code!~"5.*"}[5m])) /  sum(irate(istio_requests_total{reporter="source", source_workload="istio-ingressgateway",source_workload_namespace="istio-system", connection_security_policy!="mutual_tls"}[5m])) < 0.995
      for: 5m
      labels:
        severity: warning
        controlPlane: true
  prometheus-alerts:
    # PrometheusTargetMissing:
    #   expr: up == 0
    #   for: 0m
    #   labels:
    #     severity: critical
    #     controlPlane: true
    #   annotations:
    #     summary: "Prometheus target missing (instance {{ $labels.instance }})"
    #     description: "A Prometheus target has disappeared. An exporter might be crashed.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusAllTargetsMissing:
      expr: count by (job) (up) == 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus all targets missing (instance {{ $labels.instance }})"
        description: "A Prometheus job does not have living target anymore.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusConfigurationReloadFailure:
      expr: prometheus_config_last_reload_successful != 1
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Prometheus configuration reload failure (instance {{ $labels.instance }})"
        description: "Prometheus configuration reload error\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTooManyRestarts:
      expr: changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m]) > 2
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Prometheus too many restarts (instance {{ $labels.instance }})"
        description: "Prometheus has restarted more than twice in the last 15 minutes. It might be crashlooping.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusAlertmanagerConfigurationReloadFailure:
      expr: alertmanager_config_last_reload_successful != 1
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Prometheus AlertManager configuration reload failure (instance {{ $labels.instance }})"
        description: "AlertManager configuration reload error\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusAlertmanagerConfigNotSynced:
      expr: count(count_values("config_hash", alertmanager_config_hash)) > 1
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Prometheus AlertManager config not synced (instance {{ $labels.instance }})"
        description: "Configurations of AlertManager cluster instances are out of sync\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusNotConnectedToAlertmanager:
      expr: prometheus_notifications_alertmanagers_discovered < 1
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus not connected to alertmanager (instance {{ $labels.instance }})"
        description: "Prometheus cannot connect the alertmanager\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    # PrometheusRuleEvaluationFailures:
    #   expr: increase(prometheus_rule_evaluation_failures_total[3m]) > 0
    #   for: 0m
    #   labels:
    #     severity: critical
    #   annotations:
    #     summary: "Prometheus rule evaluation failures (instance {{ $labels.instance }})"
    #     description: "Prometheus encountered {{ $value }} rule evaluation failures, leading to potentially ignored alerts.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTemplateTextExpansionFailures:
      expr: increase(prometheus_template_text_expansion_failures_total[3m]) > 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus template text expansion failures (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} template text expansion failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusRuleEvaluationSlow:
      expr: prometheus_rule_group_last_duration_seconds > prometheus_rule_group_interval_seconds
      for: 5m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Prometheus rule evaluation slow (instance {{ $labels.instance }})"
        description: "Prometheus rule evaluation took more time than the scheduled interval. It indicates a slower storage backend access or too complex query.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusNotificationsBacklog:
      expr: min_over_time(prometheus_notifications_queue_length[10m]) > 0
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Prometheus notifications backlog (instance {{ $labels.instance }})"
        description: "The Prometheus notification queue has not been empty for 10 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusAlertmanagerNotificationFailing:
      expr: rate(alertmanager_notifications_failed_total[1m]) > 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus AlertManager notification failing (instance {{ $labels.instance }})"
        description: "Alertmanager is failing sending notifications\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTargetEmpty:
      expr: prometheus_sd_discovered_targets{config!="monitoring/vmagent/0"} == 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus target empty (instance {{ $labels.instance }})"
        description: "Prometheus has no target in service discovery\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTargetScrapingSlow:
      expr: prometheus_target_interval_length_seconds{quantile="0.9", interval!="30m0s"} > 60
      for: 5m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Prometheus target scraping slow (instance {{ $labels.instance }})"
        description: "Prometheus is scraping exporters slowly\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusLargeScrape:
      expr: increase(prometheus_target_scrapes_exceeded_sample_limit_total[10m]) > 10
      for: 5m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Prometheus large scrape (instance {{ $labels.instance }})"
        description: "Prometheus has many scrapes that exceed the sample limit\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTargetScrapeDuplicate:
      expr: increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Prometheus target scrape duplicate (instance {{ $labels.instance }})"
        description: "Prometheus has many samples rejected due to duplicate timestamps but different values\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTsdbCheckpointCreationFailures:
      expr: increase(prometheus_tsdb_checkpoint_creations_failed_total[1m]) > 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus TSDB checkpoint creation failures (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} checkpoint creation failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTsdbCheckpointDeletionFailures:
      expr: increase(prometheus_tsdb_checkpoint_deletions_failed_total[1m]) > 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus TSDB checkpoint deletion failures (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} checkpoint deletion failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTsdbCompactionsFailed:
      expr: increase(prometheus_tsdb_compactions_failed_total[1m]) > 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus TSDB compactions failed (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} TSDB compactions failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTsdbHeadTruncationsFailed:
      expr: increase(prometheus_tsdb_head_truncations_failed_total[1m]) > 0
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "Prometheus TSDB head truncations failed (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} TSDB head truncation failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTsdbReloadFailures:
      expr: increase(prometheus_tsdb_reloads_failures_total[1m]) > 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus TSDB reload failures (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} TSDB reload failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTsdbWalCorruptions:
      expr: increase(prometheus_tsdb_wal_corruptions_total[1m]) > 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus TSDB WAL corruptions (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} TSDB WAL corruptions\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTsdbWalTruncationsFailed:
      expr: increase(prometheus_tsdb_wal_truncations_failed_total[1m]) > 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus TSDB WAL truncations failed (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} TSDB WAL truncation failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusBadConfig:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has failed to reload its configuration.
        summary: Failed Prometheus configuration reload.
      expr: |-
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(prometheus_config_last_reload_successful[5m]) == 0
      for: 10m
      labels:
        severity: critical
        controlPlane: true
    PrometheusNotificationQueueRunningFull:
      annotations:
        description: Alert notification queue of Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is running full.
        summary: Prometheus alert notification queue predicted to run full in less than 30m.
      expr: |-
        # Without min_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          predict_linear(prometheus_notifications_queue_length[5m], 60 * 30)
        >
          min_over_time(prometheus_notifications_queue_capacity[5m])
        )
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    PrometheusErrorSendingAlertsToSomeAlertmanagers:
      annotations:
        description: '{{  printf "%.1f" $value  }}% errors while sending alerts from Prometheus {{ $labels.namespace }}/{{ $labels.pod }} to Alertmanager {{ $labels.alertmanager }}.'
        summary: Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager.
      expr: |-
        (
          rate(prometheus_notifications_errors_total[5m])
        /
          rate(prometheus_notifications_sent_total[5m])
        )
        * 100
        > 1
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    PrometheusErrorSendingAlertsToAnyAlertmanager:
      annotations:
        description: '{{  printf "%.1f" $value  }}% minimum errors while sending alerts from Prometheus {{ $labels.namespace }}/{{ $labels.pod }} to any Alertmanager.'
        summary: Prometheus encounters more than 3% errors sending alerts to any Alertmanager.
      expr: |-
        min without(alertmanager) (
          rate(prometheus_notifications_errors_total[5m])
        /
          rate(prometheus_notifications_sent_total[5m])
        )
        * 100
        > 3
      for: 15m
      labels:
        severity: critical
        controlPlane: true
    PrometheusNotConnectedToAlertmanagers:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is not connected to any Alertmanagers.
        summary: Prometheus is not connected to any Alertmanagers.
      expr: |-
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(prometheus_notifications_alertmanagers_discovered[5m]) < 1
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    PrometheusTSDBReloadsFailing:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has detected {{ $value | humanize }} reload failures over the last 3h.
        summary: Prometheus has issues reloading blocks from disk.
      expr: increase(prometheus_tsdb_reloads_failures_total[3h]) > 0
      for: 4h
      labels:
        severity: warning
        controlPlane: true
    PrometheusTSDBCompactionsFailing:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has detected {{ $value | humanize }} compaction failures over the last 3h.
        summary: Prometheus has issues compacting blocks.
      expr: increase(prometheus_tsdb_compactions_failed_total[3h]) > 0
      for: 4h
      labels:
        severity: warning
        controlPlane: true
    PrometheusNotIngestingSamples:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is not ingesting samples.
        summary: Prometheus is not ingesting samples.
      expr: rate(prometheus_tsdb_head_samples_appended_total[5m]) <= 0
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    PrometheusDuplicateTimestamps:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is dropping {{  printf "%.4g" $value   }} samples/s with different values but duplicated timestamp.
        summary: Prometheus is dropping samples with duplicate timestamps.
      expr: rate(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    PrometheusOutOfOrderTimestamps:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is dropping {{  printf "%.4g" $value   }} samples/s with timestamps arriving out of order.
        summary: Prometheus drops samples with out-of-order timestamps.
      expr: rate(prometheus_target_scrapes_sample_out_of_order_total[5m]) > 0
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    PrometheusRemoteStorageFailures:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} failed to send {{  printf "%.1f" $value  }}% of the samples to {{  $labels.remote_name }}:{{  $labels.url  }}
        summary: Prometheus fails to send samples to remote storage.
      expr: |-
        (
          rate(prometheus_remote_storage_failed_samples_total[5m])
        /
          (
            rate(prometheus_remote_storage_failed_samples_total[5m])
          +
            rate(prometheus_remote_storage_succeeded_samples_total[5m])
          )
        )
        * 100
        > 1
      for: 15m
      labels:
        severity: critical
        controlPlane: true
    PrometheusRemoteWriteBehind:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} remote write is {{  printf "%.1f" $value  }}s behind for {{  $labels.remote_name }}:{{  $labels.url  }}.
        summary: Prometheus remote write is behind.
      expr: |-
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds[5m])
        - on(job, instance) group_right
          max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds[5m])
        )
        > 120
      for: 15m
      labels:
        severity: critical
        controlPlane: true
    PrometheusRemoteWriteDesiredShards:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} remote write desired shards calculation wants to run {{  $value  }} shards for queue {{  $labels.remote_name }}:{{  $labels.url  }}, which is more than the max of {{  printf `prometheus_remote_storage_shards_max{instance="%s"}` $labels.instance | query | first | value  }}.
        summary: Prometheus remote write desired shards calculation wants to run more than configured max shards.
      expr: |-
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          max_over_time(prometheus_remote_storage_shards_desired[5m])
        >
          max_over_time(prometheus_remote_storage_shards_max[5m])
        )
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    PrometheusRuleFailures:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has failed to evaluate {{  printf "%.0f" $value  }} rules in the last 5m.
        summary: Prometheus is failing rule evaluations.
      expr: increase(prometheus_rule_evaluation_failures_total[5m]) > 0
      for: 15m
      labels:
        severity: critical
        controlPlane: true
    PrometheusMissingRuleEvaluations:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has missed {{  printf "%.0f" $value  }} rule group evaluations in the last 5m.
        summary: Prometheus is missing rule evaluations due to slow rule group evaluation.
      expr: increase(prometheus_rule_group_iterations_missed_total[5m]) > 0
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    PrometheusTargetLimitHit:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has dropped {{  printf "%.0f" $value  }} targets because the number of targets exceeded the configured target_limit.
        summary: Prometheus has dropped targets because some scrape configs have exceeded the targets limit.
      expr: increase(prometheus_target_scrape_pool_exceeded_target_limit_total[5m]) > 0
      for: 15m
      labels:
        severity: warning
        controlPlane: true
  prometheus-operator-alerts:
    PrometheusOperatorListErrors:
      annotations:
        description: Errors while performing List operations in controller {{ $labels.controller }} in {{ $labels.namespace }} namespace.
        summary: Errors while performing list operations in controller.
      expr: (sum by (controller,namespace) (rate(prometheus_operator_list_operations_failed_total{job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[10m])) / sum by (controller,namespace) (rate(prometheus_operator_list_operations_total{job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[10m]))) > 0.4
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    PrometheusOperatorWatchErrors:
      annotations:
        description: Errors while performing watch operations in controller {{ $labels.controller }} in {{ $labels.namespace }} namespace.
        summary: Errors while performing watch operations in controller.
      expr: (sum by (controller,namespace) (rate(prometheus_operator_watch_operations_failed_total{job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[10m])) / sum by (controller,namespace) (rate(prometheus_operator_watch_operations_total{job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[10m]))) > 0.4
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    PrometheusOperatorSyncFailed:
      annotations:
        description: Controller {{  $labels.controller  }} in {{  $labels.namespace  }} namespace fails to reconcile {{  $value  }} objects.
        summary: Last controller reconciliation failed
      expr: min_over_time(prometheus_operator_syncs{status="failed",job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[5m]) > 0
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    PrometheusOperatorReconcileErrors:
      annotations:
        description: '{{  $value | humanizePercentage  }} of reconciling operations failed for {{  $labels.controller  }} controller in {{  $labels.namespace  }} namespace.'
        summary: Errors while reconciling controller.
      expr: (sum by (controller,namespace) (rate(prometheus_operator_reconcile_errors_total{job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[5m]))) / (sum by (controller,namespace) (rate(prometheus_operator_reconcile_operations_total{job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[5m]))) > 0.1
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    PrometheusOperatorNodeLookupErrors:
      annotations:
        description: Errors while reconciling Prometheus in {{  $labels.namespace  }} Namespace.
        summary: Errors while reconciling Prometheus.
      expr: rate(prometheus_operator_node_address_lookup_errors_total{job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[5m]) > 0.1
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    PrometheusOperatorNotReady:
      annotations:
        description: Prometheus operator in {{  $labels.namespace  }} namespace isn't ready to reconcile {{  $labels.controller  }} resources.
        summary: Prometheus operator not ready
      expr: min by(namespace, controller) (max_over_time(prometheus_operator_ready{job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[5m]) == 0)
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    PrometheusOperatorRejectedResources:
      annotations:
        description: Prometheus operator in {{  $labels.namespace  }} namespace rejected {{  printf "%0.0f" $value  }} {{  $labels.controller  }}/{{  $labels.resource  }} resources.
        summary: Resources rejected by Prometheus operator
      expr: min_over_time(prometheus_operator_managed_resources{state="rejected",job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[5m]) > 0
      for: 5m
      labels:
        severity: warning
        controlPlane: true
  k8s-node-alerts:
    HostOutOfMemory:
      expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host out of memory (instance {{ $labels.instance }})"
        description: "Node memory is filling up (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostMemoryUnderMemoryPressure:
      expr: rate(node_vmstat_pgmajfault[1m]) > 1000
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host memory under memory pressure (instance {{ $labels.instance }})"
        description: "The node is under heavy memory pressure. High rate of major page faults\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostUnusualNetworkThroughputIn:
      expr: sum by (instance) (rate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 300
      for: 5m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host unusual network throughput in (instance {{ $labels.instance }})"
        description: "Host network interfaces are probably receiving too much data (> 300 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostUnusualNetworkThroughputOut:
      expr: sum by (instance) (rate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 300
      for: 5m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host unusual network throughput out (instance {{ $labels.instance }})"
        description: "Host network interfaces are probably sending too much data (> 300 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostUnusualDiskReadRate:
      expr: sum by (instance) (rate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 100
      for: 5m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host unusual disk read rate (instance {{ $labels.instance }})"
        description: "Disk is probably reading too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostUnusualDiskWriteRate:
      expr: sum by (instance) (rate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 100
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host unusual disk write rate (instance {{ $labels.instance }})"
        description: "Disk is probably writing too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    # Please add ignored mountpoints in node_exporter parameters like
    # "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|run)($|/)".
    # Same rule using "node_filesystem_free_bytes" will fire when disk fills for non-root users.
    HostOutOfDiskSpace:
      expr: (node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host out of disk space (instance {{ $labels.instance }})"
        description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    # Please add ignored mountpoints in node_exporter parameters like
    # "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|run)($|/)".
    # Same rule using "node_filesystem_free_bytes" will fire when disk fills for non-root users.
    HostDiskWillFillIn24Hours:
      expr: (node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) predict_linear(node_filesystem_avail_bytes{fstype!~"tmpfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host disk will fill in 24 hours (instance {{ $labels.instance }})"
        description: "Filesystem is predicted to run out of space within the next 24 hours at current write rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostOutOfInodes:
      expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint="/rootfs"} * 100 < 10 and ON (instance, device, mountpoint) node_filesystem_readonly{mountpoint="/rootfs"} == 0
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host out of inodes (instance {{ $labels.instance }})"
        description: "Disk is almost running out of available inodes (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostInodesWillFillIn24Hours:
      expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint="/rootfs"} * 100 < 10 and predict_linear(node_filesystem_files_free{mountpoint="/rootfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint) node_filesystem_readonly{mountpoint="/rootfs"} == 0
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host inodes will fill in 24 hours (instance {{ $labels.instance }})"
        description: "Filesystem is predicted to run out of inodes within the next 24 hours at current write rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostUnusualDiskReadLatency:
      expr: rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 0.1 and rate(node_disk_reads_completed_total[1m]) > 0
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host unusual disk read latency (instance {{ $labels.instance }})"
        description: "Disk latency is growing (read operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostUnusualDiskWriteLatency:
      expr: rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 0.1 and rate(node_disk_writes_completed_total[1m]) > 0
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host unusual disk write latency (instance {{ $labels.instance }})"
        description: "Disk latency is growing (write operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostHighCpuLoadWarning:
      expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 85
      for: 5m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host high CPU load (instance {{ $labels.instance }})"
        description: "CPU load is > 85%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostHighCpuLoadCritical:
      expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 95
      for: 5m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Host high CPU load (instance {{ $labels.instance }})"
        description: "CPU load is > 95%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostCpuStealNoisyNeighbor:
      expr: avg by(instance) (rate(node_cpu_seconds_total{mode="steal"}[5m])) * 100 > 10
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host CPU steal noisy neighbor (instance {{ $labels.instance }})"
        description: "CPU steal is > 10%. A noisy neighbor is killing VM performances or a spot instance may be out of credit.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    # 1000 context switches is an arbitrary number.
    # Alert threshold depends on nature of application.
    # Please read: https://github.com/samber/awesome-prometheus-alerts/issues/58
    HostContextSwitching:
      expr: (rate(node_context_switches_total[5m])) / (count without(cpu, mode) (node_cpu_seconds_total{mode="idle"})) > 7000
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host context switching (instance {{ $labels.instance }})"
        description: "Context switching is growing on node (> 7000 / s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostSwapIsFillingUp:
      expr: (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host swap is filling up (instance {{ $labels.instance }})"
        description: "Swap is filling up (>80%)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostSystemdServiceCrashed:
      expr: node_systemd_unit_state{state="failed"} == 1
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host SystemD service crashed (instance {{ $labels.instance }})"
        description: "SystemD service crashed\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostPhysicalComponentTooHot:
      expr: node_hwmon_temp_celsius > 75
      for: 5m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host physical component too hot (instance {{ $labels.instance }})"
        description: "Physical hardware component too hot\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostNodeOvertemperatureAlarm:
      expr: node_hwmon_temp_crit_alarm_celsius == 1
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Host node overtemperature alarm (instance {{ $labels.instance }})"
        description: "Physical node temperature alarm triggered\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostRaidArrayGotInactive:
      expr: node_md_state{state="inactive"} > 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Host RAID array got inactive (instance {{ $labels.instance }})"
        description: "RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostRaidDiskFailure:
      expr: node_md_disks{state="failed"} > 0
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host RAID disk failure (instance {{ $labels.instance }})"
        description: "At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostKernelVersionDeviations:
      expr: count(sum(label_replace(node_uname_info, "kernel", "$1", "release", "([0-9]+.[0-9]+.[0-9]+).*")) by (kernel)) > 1
      for: 6h
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host kernel version deviations (instance {{ $labels.instance }})"
        description: "Different kernel versions are running\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostOomKillDetected:
      expr: increase(node_vmstat_oom_kill[1m]) > 0
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host OOM kill detected (instance {{ $labels.instance }})"
        description: "OOM kill detected\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostEdacCorrectableErrorsDetected:
      expr: increase(node_edac_correctable_errors_total[1m]) > 0
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host EDAC Correctable Errors detected (instance {{ $labels.instance }})"
        description: "{{ $labels.instance }} has had {{ printf \"%.0f\" $value }} correctable memory errors reported by EDAC in the last 5 minutes.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostEdacUncorrectableErrorsDetected:
      expr: node_edac_uncorrectable_errors_total > 0
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host EDAC Uncorrectable Errors detected (instance {{ $labels.instance }})"
        description: "{{ $labels.instance }} has had {{ printf \"%.0f\" $value }} uncorrectable memory errors reported by EDAC in the last 5 minutes.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostNetworkReceiveErrors:
      expr: rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host Network Receive Errors (instance {{ $labels.instance }})"
        description: "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last five minutes.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostNetworkTransmitErrors:
      expr: rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host Network Transmit Errors (instance {{ $labels.instance }})"
        description: "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last five minutes.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostNetworkInterfaceSaturated:
      expr: (rate(node_network_receive_bytes_total{device!~"^tap.*"}[1m]) + rate(node_network_transmit_bytes_total{device!~"^tap.*"}[1m])) / node_network_speed_bytes{device!~"^tap.*"} > 0.8
      for: 1m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host Network Interface Saturated (instance {{ $labels.instance }})"
        description: "The network interface {{ $labels.interface }} on {{ $labels.instance }} is getting overloaded.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostConntrackLimit:
      expr: node_nf_conntrack_entries / node_nf_conntrack_entries_limit > 0.8
      for: 5m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host conntrack limit (instance {{ $labels.instance }})"
        description: "The number of conntrack is approching limit\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostClockSkew:
      expr: (node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0) or (node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0)
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host clock skew (instance {{ $labels.instance }})"
        description: "Clock skew detected. Clock is out of sync.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostClockNotSynchronising:
      expr: min_over_time(node_timex_sync_status[1m]) == 0 and node_timex_maxerror_seconds >= 16
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host clock not synchronising (instance {{ $labels.instance }})"
        description: "Clock not synchronising.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    K8SNodeNotReady:
      expr: kube_node_status_condition{condition!="Ready", status="true"} == 1
      for: 30m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: "Node has been unready for more than an 1h because of {{ $labels.condition }}."
        summary: "Node is not in optimal state."
    K8SNodePodNumberUtilization:
      expr: |
          (node:node_running_pod_count:sum / on(kubernetes_cluster, instance_name) node:node_status_pods_capacity: * 100) > 95
      for: 30m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Nodes number of POD ultilization has been over 95% for more than 5m."
        summary: "Too many pods"
    K8SNodeCPUSaturation:
      expr: |
        (sum by (kubernetes_cluster, instance_name)(node_load1{job=~"kubernetes_.+"}) / node:cpu:sum * 100) > 120
      for: 10m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Node CPU load has been saturated over 100% for more than 5m."
        summary: "CPU requests cannot be fulfilled because of unavailability.  "
    K8SNodeCPUUtilization:
      expr: |
        (node:node_container_cpu_usage_seconds:irate1m /  node:node_allocatable_cpu_cores:sum) * 100 > 90
      for: 5m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Node CPU ultilization by containers has been over 90% for more than 5m."
        summary: "Node CPU ultilization by containers is high. Recalculate CPU request and limits."
    K8SNodeMemoryUtilization:
      expr: |
        (node:node_container_memory_wss_bytes:sum / ON (kubernetes_cluster, instance_name) node:node_allocatable_memory_bytes:sum * 100) > 90
      for: 5m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Node memory ultilization by containers has been over 90% for more than 5m."
        summary: "Node memory ultilization by containers is high. Recalculate Memory request and limits."
    K8SNodeNetworkErrors:
      expr: |
        ((rate(node_network_transmit_errs_total{job=~"kubernetes_.+", device!~"veth.+"}[1m]) == 0) + (rate(node_network_receive_errs_total{job=~"kubernetes_.+", device!~"veth.+"}[1m]) == 0)) > 0
      for: 1m
      labels:
        controlPlane: true
        severity: warning
      annotations:
        description: "Node network interface {{ $labels.device }} showing errors."
        summary: "Network errors deteced on interface."
    K8SNodeNetworkInterfaceFlapping:
      expr: changes(node_network_up{job="kubernetes_.+", interface!~"veth.+"}[1m]) > 2
      for: 1m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Node network interface {{ $labels.interface }} flapping."
        summary: "Network interface flapping detected."
    K8SNodeFileSystemRunningFull24H:
      expr: (node:node_filesystem_usage:ratio > 0.85) and (predict_linear(node:node_filesystem_avail:ratio[6h], 3600 * 24) < 0)
      for: 30m
      labels:
        controlPlane: true
        severity: warning
      annotations:
        description: "Node filesystem will be full within the next 24 hours."
        summary: "Filesystem on node running out of disk."
    K8SNodeFileSystemRunningFull2H:
      expr: (node:node_filesystem_usage:ratio > 0.85) and (predict_linear(node:node_filesystem_avail:ratio[6h], 3600 * 2) < 0)
      for: 30m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Node filesystem will be full within the next 2 hours."
        summary: "Filesystem on node running out of disk."
  kube-state-metrics-alerts:
    KubernetesNodeReady:
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 10m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Kubernetes Node ready (instance {{ $labels.instance }})"
        description: "Node {{ $labels.node }} has been unready for a long time\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesMemoryPressure:
      expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
      for: 2m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Kubernetes memory pressure (instance {{ $labels.instance }})"
        description: "{{ $labels.node }} has MemoryPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesDiskPressure:
      expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
      for: 2m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Kubernetes disk pressure (instance {{ $labels.instance }})"
        description: "{{ $labels.node }} has DiskPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesOutOfDisk:
      expr: kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
      for: 2m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Kubernetes out of disk (instance {{ $labels.instance }})"
        description: "{{ $labels.node }} has OutOfDisk condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesOutOfCapacity:
      expr: sum(kube_pod_info) by (node) / sum(kube_node_status_allocatable_pods) by (node) * 100 > 90
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Kubernetes out of capacity (instance {{ $labels.instance }})"
        description: "{{ $labels.node }} is out of capacity\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    # KubernetesJobFailed:
    #   expr: kube_job_status_failed > 0
    #   for: 0m
    #   labels:
    #     severity: warning
    #   annotations:
    #     summary: "Kubernetes Job failed (instance {{ $labels.instance }})"
    #     description: "Job {{$labels.namespace}}/{{$labels.exported_job}} failed to complete\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesApiServerErrors:
      expr: sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[1m])) / sum(rate(apiserver_request_count{job="apiserver"}[1m])) * 100 > 3
      for: 2m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Kubernetes API server errors (instance {{ $labels.instance }})"
        description: "Kubernetes API server is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesApiClientErrors:
      expr: (sum(rate(rest_client_requests_total{code=~"(4|5).."}[1m])) by (instance, job) / sum(rate(rest_client_requests_total[1m])) by (instance, job)) * 100 > 1
      for: 2m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Kubernetes API client errors (instance {{ $labels.instance }})"
        description: "Kubernetes API client is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesClientCertificateExpiresNextWeek:
      expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 7*24*60*60
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Kubernetes client certificate expires next week (instance {{ $labels.instance }})"
        description: "A client certificate used to authenticate to the apiserver is expiring next week.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesClientCertificateExpiresSoon:
      expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 24*60*60
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Kubernetes client certificate expires soon (instance {{ $labels.instance }})"
        description: "A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
  cluster-autoscaler-alerts:
    # ClusterAutoScalerScaling30m:
    #   expr: abs(sum(cluster_autoscaler_scaled_up_nodes_total)-sum(cluster_autoscaler_scaled_down_nodes_total)) >=1
    #   for: 30m
    #   labels:
    #     severity: warning
    #     controlPlane: true
    #   annotations:
    #     description: Too many nodes scaling in the last 30m {{ $value }} node(s)
    #     summary: Kube Cluster Autoscaler is scaling down or up in the last 30 minutes
    # ClusterAutoScalerScaling60m:
    #   expr: abs(sum(cluster_autoscaler_scaled_up_nodes_total)-sum(cluster_autoscaler_scaled_down_nodes_total)) >=1
    #   for: 60m
    #   labels:
    #     severity: warning
    #     controlPlane: true
    #   annotations:
    #     description: Too many nodes scaling in the last 60m {{ $value }} node(s)
    #     summary: Kube Cluster Autoscaler is scaling down or up in the last 60 minutes
    ClusterAutoScalerUnschedulablePods10m:
      expr: sum(cluster_autoscaler_unschedulable_pods_count) > 1
      for: 10m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        description: Pods can't be scheduled for 10 minutes {{ $value }}
        summary: Unscheduled pods for 10 minutes
    ClusterAutoScalerUnschedulablePods30m:
      expr: sum(cluster_autoscaler_unschedulable_pods_count) > 1
      for: 30m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: Pods can't be scheduled for 30 minutes {{ $value }}
        summary: Unscheduled pods for 30 minutes
    ClusterAutoScalerErrors:
      expr: increase(cluster_autoscaler_errors_total[5m]) > 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: Cluster Autoscaler has errors {{ $value }}
        summary: Cluster Autoscaler has errors
  core-dns-alerts:
    K8SCoreDNSErrorRateZScore:
      expr: 'coredns:error_rate_zscore_abs > 5'
      for: 15m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: "ZScore for CoreDNS Error rate is more than 5 for 15 min"
    K8SCoreDNSRequestsZScore:
      expr: 'coredns:requests_rate_zscore_abs > 5'
      for: 15m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: "ZScore for CoreDNS Request rate is more than 5 for 15 min"
    K8SCoreDNSCacheHitZScore:
      expr: 'coredns:cache_hits_rate_zscore_abs > 5'
      for: 15m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: "ZScore for CoreDNS Cache Hit rate is more than 5 for 15 min"
  k8s-cluster-alerts:
    K8SStateMetricsDown:
      expr: absent(up{container=~".*state-metrics.*"} == 1)
      for: 10m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "KubeStateMetrics has disappeared from Prometheus target discovery for longer than 10m."
        summary: "KubeStateMetrics is probably down."
    K8SVersionMismatch:
      expr: count by(kubernetes_cluster)(count by (kubernetes_cluster, gitVersion) (label_replace(kubernetes_build_info{job=~"kubernetes_node.+"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*.[0-9]*).*"))) > 1
      for: 1h
      labels:
        controlPlane: true
        severity: warning
      annotations:
        description: "There has been different semantic versions of Kubernetes components running for more than 1h."
        summary: "NodeExporter is probably down."
    K8SClusterCPUOvercommit:
      expr: sum by(kubernetes_cluster) (namespace:kube_pod_container_resource_requests_cpu_cores:sum) / sum by(kubernetes_cluster) (kube_node_status_allocatable_cpu_cores{job=~"kubernetes_.+"}) - (sum by(kubernetes_cluster) (kube_node_status_allocatable_cpu_cores) - 1) / sum by(kubernetes_cluster) (kube_node_status_allocatable_cpu_cores) > 0
      for: 1h
      labels:
        controlPlane: true
        severity: warning
      annotations:
        description: "Cluster has overcommitted CPU resource requests for Pods and cannot tolerate node failure for more than 1h."
        summary: ""
    K8SClusterMemoryOvercommit:
      expr: |
        sum by (kubernetes_cluster)(namespace:kube_pod_container_resource_requests_memory_bytes:sum)
            /
          sum by (kubernetes_cluster)(kube_node_status_allocatable_memory_bytes{job=~"kubernetes_.+"})
            -
          (sum by (kubernetes_cluster)(kube_node_status_allocatable_memory_bytes)-1) / sum by (kubernetes_cluster)(kube_node_status_allocatable_memory_bytes) > 0
      for: 1h
      labels:
        controlPlane: true
        severity: warning
      annotations:
        description: "Cluster has overcommitted memory resource requests for Pods and cannot tolerate node failure for more than 1h."
        summary: ""
    K8SClusterClientCertificateExpiration:
      expr: |
        apiserver_client_certificate_expiration_seconds_count{job=~"kubernetes_.+"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=~"kubernetes_.+"}[5m]))) < 86400
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Kubernetes client certificate used to authenticate to the apiserver is expiring in less than 2 hours"
        summary: ""
  k8s-namespace-alerts:
    K8SNamespaceCPUOvercommit:
      expr: |
        (sum by(kubernetes_cluster, namespace, resourcequota)(kube_resourcequota{job=~"kubernetes_.+", type="hard", resource="cpu"})
        / on (kubernetes_cluster) group_left()
        sum by(kubernetes_cluster)(kube_node_status_allocatable_cpu_cores)) * 100 > 100
      for: 10m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Namespace has overcommitted CPU resource requests for longer than 10m."
    K8SNamespaceMemoryOvercommit:
      expr: |
          (sum by(kubernetes_cluster, namespace, resourcequota)(kube_resourcequota{job=~"kubernetes_.+", type="hard", resource="memory"})
            / on (kubernetes_cluster) group_left()
          sum by(kubernetes_cluster)(kube_node_status_allocatable_memory_bytes)) * 100
            > 100
      for: 10m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Namespace has overcommitted memory resource requests for longer than 10m."
    K8SNamespaceQuotaExceeded:
      expr: |
          (kube_resourcequota{job=~"kubernetes_.+", type="used"}
            / ignoring(instance, job, type)
          (kube_resourcequota{job=~"kubernetes_.+", type="hard"} > 0)) * 100
            > 90
      for: 10m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Namespace has been using more resource than 90% of quota for longer than 10m."
  kubernetes-storage-alerts:
    K8SPersistentVolumeRunningFull24H:
      expr: (kubelet:persistent_volume_usage:ratio > 0.85) and (predict_linear(kubelet:persistent_volume_avail:ratio[6h], 3600 * 24) < 0)
      for: 30m
      labels:
        severity: warning
      annotations:
        description: "PersistentVolume will be full within the next 24 hours."
    K8SPersistentVolumeRunningFull2H:
      expr: (kubelet:persistent_volume_usage:ratio > 0.85) and (predict_linear(kubelet:persistent_volume_avail:ratio[6h], 3600 * 2) < 0)
      for: 30m
      labels:
        severity: critical
      annotations:
        description: "PersistentVolume will be full within the next 2 hours."
    K8SPersistentVolumeInodesRunningFull24H:
      expr: (kubelet:persistent_volume_inode_usage:ratio > 0.85) and (predict_linear(kubelet:persistent_volume_inode_avail:ratio[6h], 3600 * 24) < 0)
      for: 30m
      labels:
        severity: warning
      annotations:
        description: "PersistentVolume inode will reached limit within the next 24 hours."
    K8SPersistentVolumeInodesRunningFull2H:
      expr: (kubelet:persistent_volume_inode_usage:ratio > 0.85) and (predict_linear(kubelet:persistent_volume_inode_avail:ratio[6h], 3600 * 24) < 0)
      for: 30m
      labels:
        severity: critical
      annotations:
        description: "PersistentVolume inode will reached limit within the next 2 hours."
    K8SPersistentVolumePendingState:
      expr: kube_persistentvolume_status_phase{job=~"kubernetes_.+", phase="Pending"} > 0
      for: 10m
      labels:
        severity: critical
      annotations:
        description: "PersistentVolume has been in pending state for longer than 10m."
    K8SPersistentVolumeFailedState:
      expr: kube_persistentvolume_status_phase{job=~"kubernetes_.+", phase="Failed"} > 0
      for: 10m
      labels:
        severity: critical
      annotations:
        description: "PersistentVolume has been in failed state for longer than 10m."
    KubePersistentVolumeErrors:
      annotations:
        description: The persistent volume {{  $labels.persistentvolume  }} has status {{  $labels.phase  }}.
        summary: PersistentVolume is having issues with provisioning.
      expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} > 0
      for: 5m
      labels:
        severity: critical
  k8s-workload-alerts:
    K8SPodContainerCrashLooping:
      expr: rate(kube_pod_container_status_restarts_total[10m]) * 600 > 0
      for: 15m
      labels:
        severity: critical
      annotations:
        description: "Pod container is in crash loop for longer than 15m."
        summary: "This means that one of the containers in the pod has exited unexpectedly, and perhaps with a non-zero error code even after restarting."
    K8SPodContainerErrImagePull:
      expr: kube_pod_container_status_waiting_reason{reason="ErrImagePull"} > 0
      for: 10m
      labels:
        severity: critical
      annotations:
        description: "Pod container has an {{ $labels.reason}} for longer than 10m."
        summary: "General image pull error"
    K8SPodContainerImagePullBackOff:
      expr: kube_pod_container_status_waiting_reason{reason="ImagePullBackOff"} > 0
      for: 10m
      labels:
        severity: critical
      annotations:
        description: "Pod container has an {{ $labels.reason}} for longer than 10m."
        summary: "Container image pull failed, kubelet is backing off image pull"
    K8SPodContainerCreatingStuck:
      expr: kube_pod_container_status_waiting_reason{reason="ContainerCreating"} > 0
      for: 10m
      labels:
        severity: critical
      annotations:
        description: "Pod container has an {{ $labels.reason}} for longer than 10m."
        summary: "Container creating stuck"
    K8SPodContainerCrashLoopBackOff:
      expr: kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff"} > 0
      for: 10m
      labels:
        severity: critical
      annotations:
        description: "Pod container has an {{ $labels.reason}} for longer than 10m."
        summary: "Container Terminated and Kubelet is backing off the restart"
    K8SPodContainerCPURequests:
      expr: label_join(rate(container_cpu_user_seconds_total{namespace!="kube-system",container!="POD",container!=""}[5m]), "pod_container", "/" , "pod", "container") * 100 / on (kubernetes_cluster, namespace, pod_container) group_left label_join(kube_pod_container_resource_requests_cpu_cores{namespace!="kube-system",container!=""}, "pod_container", "/" , "pod", "container") >= 150
      for: 10m
      labels:
        severity: warning
      annotations:
        description: "Pod container CPU usage has been 150% over CPU requests for longer than 10m."
        summary: "Pod container CPU requests should be increased"
    K8SPodContainerCPULimits:
      expr: label_join(rate(container_cpu_user_seconds_total{namespace!="kube-system",container!="POD",container!=""}[5m]), "pod_container", "/" , "pod", "container") * 100 / on (kubernetes_cluster, namespace, pod_container)  group_left label_join(kube_pod_container_resource_limits_cpu_cores{namespace!="kube-system",container!=""}, "pod_container", "/" , "pod", "container") >= 95
      for: 10m
      labels:
        severity: critical
      annotations:
        description: "Pod container CPU usage has been more ultilized than 95% of CPU limits for longer than 10m."
        summary: "Pod container CPU limits should be increased"
    # K8SPodContainerCPUThrotlling:
    #   expr: increase(container_cpu_cfs_throttled_periods_total{namespace!="kube-system",container!="POD",container!=""}[1m]) * 100 / on (id) group_left increase(container_cpu_cfs_periods_total{namespace!="kube-system",container!="POD",container!=""}[1m]) > 50
    #   for: 15m
    #   labels:
    #     severity: warning
    #   annotations:
    #     description: "Pod container CPU throtlling has been more than 50% for longer than 15m."
    #     summary: "Pod container CPU limits should be increased"
    K8SPodContainerMemRequests:
      expr: label_join(container_memory_working_set_bytes{namespace!="kube-system",container!="POD",container!=""}, "pod_container", "/" , "pod", "container") * 100 / on (kubernetes_cluster, namespace, pod_container)  group_left label_join(kube_pod_container_resource_requests_memory_bytes{namespace!="kube-system",container!="POD",container!=""}, "pod_container", "/" , "pod", "container") >= 150
      for: 30m
      labels:
        severity: warning
      annotations:
        description: "Pod container Mem usage has been 150% over Mem requests for longer than 30m."
        summary: "Pod container Mem requests should be increased"
    K8SPodContainerMemLimits:
      expr: label_join(container_memory_working_set_bytes{namespace!="kube-system",container!="POD",container!=""}, "pod_container", "/" , "pod", "container") * 100 / on (kubernetes_cluster, namespace, pod_container)  group_left label_join(kube_pod_container_resource_limits_memory_bytes{namespace!="kube-system",container!="POD",container!=""}, "pod_container", "/" , "pod", "container") > 95
      for: 10m
      labels:
        severity: critical
      annotations:
        description: "Pod container Mem usage has been more ultilized than 95% of Mem limits for longer than 5m."
        summary: "Pod container Mem limits should be increased"
    K8SPodContainerFSSizeUsage:
      expr: container_fs_usage_bytes{container!="POD",container!=""} *100 / container_fs_limit_bytes{container!="POD",container!=""} > 90
      for: 30m
      labels:
        severity: critical
      annotations:
        description: "Pod container filesystem usage has been more than 90% of limits for longer than 30m."
        summary: "Pod container Mem limits should be increased"
    K8SPodPendingState:
      expr: sum by (kubernetes_cluster, namespace, pod, phase) (kube_pod_status_phase{phase="Pending"}) > 0
      for: 30m
      labels:
        severity: critical
      annotations:
        description: "Pod has been in pending state for longer than 30m."
        summary: "The Pod has been accepted by the Kubernetes system, but one or more of the Container images has not been created. This includes time before being scheduled as well as time spent downloading images over the network, which could take a while."
    K8SPodUnknownState:
      expr: sum by (kubernetes_cluster, namespace, pod, phase) (kube_pod_status_phase{phase="Unknown"}) > 0
      for: 30m
      labels:
        severity: critical
      annotations:
        description: "Pod has been in unknown state for longer than 30m."
        summary: "For some reason the state of the Pod could not be obtained, typically due to an error in communicating with the host of the Pod."
    K8SDeploymentAvailableReplicasMismatch:
      expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
      for: 30m
      labels:
        severity: critical
      annotations:
        description: "Deployment has not the expected number of available replicas for longer than 30m."
        summary: ""
    K8SDeploymentUpdatedReplicasMismatch:
      expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_updated
      for: 30m
      labels:
        severity: critical
      annotations:
        description: "Deployment has not the expected number of updated replicas for longer than 30m."
        summary: ""
    K8SDeploymentPending:
      expr: kube_deployment_status_observed_generation != kube_deployment_metadata_generation
      for: 15m
      labels:
        severity: critical
      annotations:
        description: "Deployment generation does not match for longer than 15m"
        summary: "Deployment has failed but has not been rolled back"
    K8SStatefulSetAvailableReplicasMismatch:
      expr: kube_statefulset_status_replicas != kube_statefulset_status_replicas_ready
      for: 15m
      labels:
        severity: critical
      annotations:
        description: "StatefulSet has not the expected number of ready replicas for longer than 15m."
    K8SStatefulSetUpdateNotRolledOut:
      expr: |
        max without (revision) (
            kube_statefulset_status_current_revision{job=~"kubernetes_.+"}
              unless
            kube_statefulset_status_update_revision{job=~"kubernetes_.+"}
            )
              *
            (
              kube_statefulset_replicas{job=~"kubernetes_.+"}
                !=
              kube_statefulset_status_replicas_updated{job=~"kubernetes_.+"}
            )
      for: 15m
      labels:
        severity: critical
      annotations:
        description: "StatefulSet has not been rolled out for longer than 15m."
    K8SStatefulSetPending:
      expr: kube_statefulset_status_observed_generation != kube_statefulset_metadata_generation
      for: 15m
      labels:
        severity: critical
      annotations:
        description: "StatefulSet generation does not match for longer than 15m."
    K8SDaemonSetAvailablePodsMismatch:
      expr: kube_daemonset_status_number_ready != kube_daemonset_status_desired_number_scheduled
      for: 15m
      labels:
        severity: critical
      annotations:
        description: "DaemonSet has not the expected number of ready replicas for longer than 15m."
    K8SDaemonSetNotScheduled:
      expr: kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0
      for: 15m
      labels:
        severity: critical
      annotations:
        description: "DaemonSet has not the expected number of desired replicas for longer than 15m."
    K8SDaemonSetMissScheduled:
      expr: kube_daemonset_status_number_misscheduled > 0
      for: 15m
      labels:
        severity: critical
      annotations:
        description: "DaemonSet has been mischeduled for longer than 15m."
    K8SJobFailed:
      expr: kube_job_status_failed{job=~"kubernetes_.+"} > 0
      for: 1h
      labels:
        severity: critical
      annotations:
        description: "Job failed to complete."
    KubePodCrashLooping:
      annotations:
        description: Pod {{  $labels.namespace  }}/{{  $labels.pod  }} ({{  $labels.container  }}) is restarting {{  printf "%.2f" $value  }} times / 5 minutes.
        summary: Pod is crash looping.
      expr: rate(kube_pod_container_status_restarts_total{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}[5m]) * 60 * 5 > 0
      for: 15m
      labels:
        severity: warning
    KubePodNotReady:
      annotations:
        description: Pod {{  $labels.namespace  }}/{{  $labels.pod  }} has been in a non-ready state for longer than 15 minutes.
        summary: Pod has been in a non-ready state for more than 15 minutes.
      expr: |-
        sum by (namespace, pod) (
          max by(namespace, pod) (
            kube_pod_status_phase{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", phase=~"Pending|Unknown"}
          ) * on(namespace, pod) group_left(owner_kind) topk by(namespace, pod) (
            1, max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"})
          )
        ) > 0
      for: 15m
      labels:
        severity: warning
    KubeDeploymentGenerationMismatch:
      annotations:
        description: Deployment generation for {{  $labels.namespace  }}/{{  $labels.deployment  }} does not match, this indicates that the Deployment has failed but has not been rolled back.
        summary: Deployment generation mismatch due to possible roll-back
      expr: |-
        kube_deployment_status_observed_generation{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
          !=
        kube_deployment_metadata_generation{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
      for: 15m
      labels:
        severity: warning
    KubeDeploymentReplicasMismatch:
      annotations:
        description: Deployment {{  $labels.namespace  }}/{{  $labels.deployment  }} has not matched the expected number of replicas for longer than 15 minutes.
        summary: Deployment has not matched the expected number of replicas.
      expr: |-
        (
          kube_deployment_spec_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
            !=
          kube_deployment_status_replicas_available{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
        ) and (
          changes(kube_deployment_status_replicas_updated{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}[5m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    KubeStatefulSetReplicasMismatch:
      annotations:
        description: StatefulSet {{  $labels.namespace  }}/{{  $labels.statefulset  }} has not matched the expected number of replicas for longer than 15 minutes.
        summary: Deployment has not matched the expected number of replicas.
      expr: |-
        (
          kube_statefulset_status_replicas_ready{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
            !=
          kube_statefulset_status_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
        ) and (
          changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}[5m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    KubeStatefulSetGenerationMismatch:
      annotations:
        description: StatefulSet generation for {{  $labels.namespace  }}/{{  $labels.statefulset  }} does not match, this indicates that the StatefulSet has failed but has not been rolled back.
        summary: StatefulSet generation mismatch due to possible roll-back
      expr: |-
        kube_statefulset_status_observed_generation{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
          !=
        kube_statefulset_metadata_generation{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
      for: 15m
      labels:
        severity: warning
    KubeStatefulSetUpdateNotRolledOut:
      annotations:
        description: StatefulSet {{  $labels.namespace  }}/{{  $labels.statefulset  }} update has not been rolled out.
        summary: StatefulSet update has not been rolled out.
      expr: |-
        (
          max without (revision) (
            kube_statefulset_status_current_revision{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
              unless
            kube_statefulset_status_update_revision{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
          )
            *
          (
            kube_statefulset_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
              !=
            kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
          )
        )  and (
          changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}[5m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    KubeDaemonSetRolloutStuck:
      annotations:
        description: DaemonSet {{  $labels.namespace  }}/{{  $labels.daemonset  }} has not finished or progressed for at least 15 minutes.
        summary: DaemonSet rollout is stuck.
      expr: |-
        (
          (
            kube_daemonset_status_current_number_scheduled{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
            !=
            kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
          ) or (
            kube_daemonset_status_number_misscheduled{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
            !=
            0
          ) or (
            kube_daemonset_updated_number_scheduled{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
            !=
            kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
          ) or (
            kube_daemonset_status_number_available{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
            !=
            kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
          )
        ) and (
          changes(kube_daemonset_updated_number_scheduled{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}[5m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    KubeContainerWaiting:
      annotations:
        description: Pod {{  $labels.namespace  }}/{{  $labels.pod  }} container {{  $labels.container }} has been in waiting state for longer than 1 hour.
        summary: Pod container waiting longer than 1 hour
      expr: sum by (namespace, pod, container) (kube_pod_container_status_waiting_reason{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}) > 0
      for: 1h
      labels:
        severity: warning
    KubeDaemonSetNotScheduled:
      annotations:
        description: '{{  $value  }} Pods of DaemonSet {{  $labels.namespace  }}/{{  $labels.daemonset  }} are not scheduled.'
        summary: DaemonSet pods are not scheduled.
      expr: |-
        kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
          -
        kube_daemonset_status_current_number_scheduled{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"} > 0
      for: 10m
      labels:
        severity: warning
    KubeDaemonSetMisScheduled:
      annotations:
        description: '{{  $value  }} Pods of DaemonSet {{  $labels.namespace  }}/{{  $labels.daemonset  }} are running where they are not supposed to run.'
        summary: DaemonSet pods are misscheduled.
      expr: kube_daemonset_status_number_misscheduled{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"} > 0
      for: 15m
      labels:
        severity: warning
    KubeJobCompletion:
      annotations:
        description: Job {{  $labels.namespace  }}/{{  $labels.job_name  }} is taking more than 12 hours to complete.
        summary: Job did not complete in time
      expr: kube_job_spec_completions{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"} - kube_job_status_succeeded{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}  > 0
      for: 12h
      labels:
        severity: warning
    KubeJobFailed:
      annotations:
        description: Job {{  $labels.namespace  }}/{{  $labels.job_name  }} failed to complete. Removing failed job after investigation should clear this alert.
        summary: Job failed to complete.
      expr: kube_job_failed{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}  > 0
      for: 15m
      labels:
        severity: warning
    KubeHpaReplicasMismatch:
      annotations:
        description: HPA {{  $labels.namespace  }}/{{  $labels.hpa  }} has not matched the desired number of replicas for longer than 15 minutes.
        summary: HPA has not matched descired number of replicas.
      expr: |-
        (kube_hpa_status_desired_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
          !=
        kube_hpa_status_current_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"})
          and
        (kube_hpa_status_current_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
          >
        kube_hpa_spec_min_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"})
          and
        (kube_hpa_status_current_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
          <
        kube_hpa_spec_max_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"})
          and
        changes(kube_hpa_status_current_replicas[15m]) == 0
      for: 15m
      labels:
        severity: warning
    KubeHpaMaxedOut:
      annotations:
        description: HPA {{  $labels.namespace  }}/{{  $labels.hpa  }} has been running at max replicas for longer than 15 minutes.
        summary: HPA is running at max replicas
      expr: |-
        kube_hpa_status_current_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
          ==
        kube_hpa_spec_max_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
      for: 15m
      labels:
        severity: warning
    KubernetesCronjobSuspended:
      expr: kube_cronjob_spec_suspend != 0
      for: 0m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes CronJob suspended (instance {{ $labels.instance }})"
        description: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is suspended\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesPersistentvolumeclaimPending:
      expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance }})"
        description: "PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesVolumeOutOfDiskSpace:
      expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes Volume out of disk space (instance {{ $labels.instance }})"
        description: "Volume is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesVolumeFullInTwoDays:
      expr: predict_linear(kubelet_volume_stats_available_bytes[6h], 2 * 24 * 3600) < 0
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes Volume full in two days (instance {{ $labels.instance }})"
        description: "{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is expected to fill up within two days. Currently {{ $value | humanize }}% is available.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesPersistentvolumeError:
      expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending", job="kube-state-metrics"} > 0
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes PersistentVolume error (instance {{ $labels.instance }})"
        description: "Persistent volume is in bad state\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesStatefulsetDown:
      expr: (kube_statefulset_status_replicas_ready / kube_statefulset_status_replicas_current) != 1
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes StatefulSet down (instance {{ $labels.instance }})"
        description: "A StatefulSet went down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesHpaScalingAbility:
      expr: kube_hpa_status_condition{status="false", condition="AbleToScale"} == 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes HPA scaling ability (instance {{ $labels.instance }})"
        description: "Pod is unable to scale\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesHpaMetricAvailability:
      expr: kube_hpa_status_condition{status="false", condition="ScalingActive"} == 1
      for: 0m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes HPA metric availability (instance {{ $labels.instance }})"
        description: "HPA is not able to collect metrics\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesHpaScaleCapability:
      expr: kube_hpa_status_desired_replicas >= kube_hpa_spec_max_replicas
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes HPA scale capability (instance {{ $labels.instance }})"
        description: "The maximum number of desired Pods has been hit\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesPodNotHealthy:
      expr: min_over_time(sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed", pod!~"kleaner.*|aws-node.*|istio-cni.*|filebeat.*|ebs-csi.*|ip-masq-agent.*|kube-proxy.*|kube-prometheus-stack-prometheus-node-exporter.*"})[1h:]) > 0
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes Pod not healthy (instance {{ $labels.instance }})"
        description: "Pod has been in a non-ready state for longer than an hour.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesPodCrashLooping:
      expr: increase(kube_pod_container_status_restarts_total[1m]) > 3
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes pod crash looping (instance {{ $labels.instance }})"
        description: "Pod {{ $labels.pod }} is crash looping\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesReplicassetMismatch:
      expr: kube_replicaset_spec_replicas != kube_replicaset_status_ready_replicas
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes ReplicasSet mismatch (instance {{ $labels.instance }})"
        description: "Deployment Replicas mismatch\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    # ubernetesDeploymentReplicasMismatch:
    #   expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
    #   for: 10m
    #   labels:
    #     severity: warning
    #   annotations:
    #     summary: "Kubernetes Deployment replicas mismatch (instance {{ $labels.instance }})"
    #     description: "Deployment Replicas mismatch\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    # KubernetesStatefulsetReplicasMismatch:
    #   expr: kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas
    #   for: 10m
    #   labels:
    #     severity: warning
    #   annotations:
    #     summary: "Kubernetes StatefulSet replicas mismatch (instance {{ $labels.instance }})"
    #     description: "A StatefulSet does not match the expected number of replicas.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesDeploymentGenerationMismatch:
      expr: kube_deployment_status_observed_generation != kube_deployment_metadata_generation
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes Deployment generation mismatch (instance {{ $labels.instance }})"
        description: "A Deployment has failed but has not been rolled back.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesStatefulsetGenerationMismatch:
      expr: kube_statefulset_status_observed_generation != kube_statefulset_metadata_generation
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes StatefulSet generation mismatch (instance {{ $labels.instance }})"
        description: "A StatefulSet has failed but has not been rolled back.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesStatefulsetUpdateNotRolledOut:
      expr: max without (revision) (kube_statefulset_status_current_revision unless kube_statefulset_status_update_revision) * (kube_statefulset_replicas != kube_statefulset_status_replicas_updated)
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes StatefulSet update not rolled out (instance {{ $labels.instance }})"
        description: "StatefulSet update has not been rolled out.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesDaemonsetRolloutStuck:
      expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 < 100 or kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes DaemonSet rollout stuck (instance {{ $labels.instance }})"
        description: "Some Pods of DaemonSet are not scheduled or not ready\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesDaemonsetMisscheduled:
      expr: kube_daemonset_status_number_misscheduled{daemonset!="filebeat"} > 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes DaemonSet misscheduled (instance {{ $labels.instance }})"
        description: "Some DaemonSet Pods are running where they are not supposed to run\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesCronjobTooLong:
      expr: time() - kube_cronjob_next_schedule_time > 3600
      for: 0m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes CronJob too long (instance {{ $labels.instance }})"
        description: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more than 1h to complete.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    # KubernetesJobSlowCompletion:
    #   expr: kube_job_spec_completions - kube_job_status_succeeded > 0
    #   for: 12h
    #   labels:
    #     severity: critical
    #   annotations:
    #     summary: "Kubernetes job slow completion (instance {{ $labels.instance }})"
    #     description: "Kubernetes Job {{ $labels.namespace }}/{{ $labels.job_name }} did not complete in time.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
  alertmanager-alerts:
    AlertmanagerConfigInconsistent:
      annotations:
        description: 'The configuration of the instances of the Alertmanager cluster `{{  $labels.namespace  }}/{{  $labels.service  }}` are out of sync.

          {{  range printf "alertmanager_config_hash{namespace=\"%s\",service=\"%s\"}" $labels.namespace $labels.service | query  }}

          Configuration hash for pod {{  .Labels.pod  }} is "{{  printf "%.f" .Value  }}"

          {{  end  }}

          '
      expr: count by(namespace,service) (count_values by(namespace,service) ("config_hash", alertmanager_config_hash{job="{{ $alertmanagerJob }}",namespace="{{ $namespace }}"})) != 1
      for: 5m
      labels:
        severity: critical
        controlPlane: true
    AlertmanagerFailedReload:
      annotations:
        description: Reloading Alertmanager's configuration has failed for {{  $labels.namespace  }}/{{  $labels.pod }}.
      expr: alertmanager_config_last_reload_successful{job="{{ $alertmanagerJob }}",namespace="{{ $namespace }}"} == 0
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    AlertmanagerMembersInconsistent:
      annotations:
        description: Alertmanager has not found all other members of the cluster.
      expr: |-
        alertmanager_cluster_members{job="{{ $alertmanagerJob }}",namespace="{{ $namespace }}"}
          != on (service) GROUP_LEFT()
        count by (service) (alertmanager_cluster_members{job="{{ $alertmanagerJob }}",namespace="{{ $namespace }}"})
      for: 5m
      labels:
        severity: critical
        controlPlane: true
  kube-apiserver-slos-alerts:
    KubeAPIErrorBudgetBurn2M:
      annotations:
        description: The API server is burning too much error budget.
        summary: The API server is burning too much error budget.
      expr: |-
        sum(apiserver_request:burnrate1h) > (14.40 * 0.01000)
        and
        sum(apiserver_request:burnrate5m) > (14.40 * 0.01000)
      for: 2m
      labels:
        severity: critical
        controlPlane: true
    KubeAPIErrorBudgetBurn15M:
      annotations:
        description: The API server is burning too much error budget.
        summary: The API server is burning too much error budget.
      expr: |-
        sum(apiserver_request:burnrate6h) > (6.00 * 0.01000)
        and
        sum(apiserver_request:burnrate30m) > (6.00 * 0.01000)
      for: 15m
      labels:
        severity: critical
        controlPlane: true
    KubeAPIErrorBudgetBurn1H:
      annotations:
        description: The API server is burning too much error budget.
        summary: The API server is burning too much error budget.
      expr: |-
        sum(apiserver_request:burnrate1d) > (3.00 * 0.01000)
        and
        sum(apiserver_request:burnrate2h) > (3.00 * 0.01000)
      for: 1h
      labels:
        severity: warning
        controlPlane: true
    KubeAPIErrorBudgetBurn3H:
      annotations:
        description: The API server is burning too much error budget.
        summary: The API server is burning too much error budget.
      expr: |-
        sum(apiserver_request:burnrate3d) > (1.00 * 0.01000)
        and
        sum(apiserver_request:burnrate6h) > (1.00 * 0.01000)
      for: 3h
      labels:
        severity: warning
        controlPlane: true
  kubernetes-resources-alerts:
    KubeCPUOvercommit:
      annotations:
        description: Cluster has overcommitted CPU resource requests for Pods and cannot tolerate node failure.
        summary: Cluster has overcommitted CPU resource requests.
      expr: |-
        sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum{})
          /
        sum(kube_node_status_allocatable_cpu_cores)
          >
        (count(kube_node_status_allocatable_cpu_cores)-1) / count(kube_node_status_allocatable_cpu_cores)
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    KubeMemoryOvercommit:
      annotations:
        description: Cluster has overcommitted memory resource requests for Pods and cannot tolerate node failure.
        summary: Cluster has overcommitted memory resource requests.
      expr: |-
        sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum{})
          /
        sum(kube_node_status_allocatable_memory_bytes)
          >
        (count(kube_node_status_allocatable_memory_bytes)-1)
          /
        count(kube_node_status_allocatable_memory_bytes)
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    KubeCPUQuotaOvercommit:
      annotations:
        description: Cluster has overcommitted CPU resource requests for Namespaces.
        summary: Cluster has overcommitted CPU resource requests.
      expr: |-
        sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="cpu"})
          /
        sum(kube_node_status_allocatable_cpu_cores)
          > 1.5
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    KubeMemoryQuotaOvercommit:
      annotations:
        description: Cluster has overcommitted memory resource requests for Namespaces.
        summary: Cluster has overcommitted memory resource requests.
      expr: |-
        sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="memory"})
          /
        sum(kube_node_status_allocatable_memory_bytes{job="kube-state-metrics"})
          > 1.5
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    KubeQuotaAlmostFull:
      annotations:
        description: Namespace {{  $labels.namespace  }} is using {{  $value | humanizePercentage  }} of its {{  $labels.resource  }} quota.
        summary: Namespace quota is going to be full.
      expr: |-
        kube_resourcequota{job="kube-state-metrics", type="used"}
          / ignoring(instance, job, type)
        (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
          > 0.9 < 1
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    KubeQuotaFullyUsed:
      annotations:
        description: Namespace {{  $labels.namespace  }} is using {{  $value | humanizePercentage  }} of its {{  $labels.resource  }} quota.
        summary: Namespace quota is fully used.
      expr: |-
        kube_resourcequota{job="kube-state-metrics", type="used"}
          / ignoring(instance, job, type)
        (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
          == 1
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    KubeQuotaExceeded:
      annotations:
        description: Namespace {{  $labels.namespace  }} is using {{  $value | humanizePercentage  }} of its {{  $labels.resource  }} quota.
        summary: Namespace quota has exceeded the limits.
      expr: |-
        kube_resourcequota{job="kube-state-metrics", type="used"}
          / ignoring(instance, job, type)
        (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
          > 1
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    CPUThrottlingHigh15Minutes:
      annotations:
        description: '{{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}, in the last 15 minutes'
        summary: Processes experience elevated CPU throttling, in the last 15 minutes
      expr: |-
        sum(increase(container_cpu_cfs_throttled_periods_total{container!="", }[5m])) by (container, pod, namespace)
          /
        sum(increase(container_cpu_cfs_periods_total{}[5m])) by (container, pod, namespace)
          > ( 25 / 100 )
      for: 15m
      labels:
        severity: warning
    CPUThrottlingHigh1Hour:
      annotations:
        description: '{{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}, in the last 1 hour'
        summary: Processes experience elevated CPU throttling, in the last 1 hour.
      expr: |-
        sum(increase(container_cpu_cfs_throttled_periods_total{container!="", }[5m])) by (container, pod, namespace)
          /
        sum(increase(container_cpu_cfs_periods_total{}[5m])) by (container, pod, namespace)
          > ( 25 / 100 )
      for: 1h
      labels:
        severity: critical
  kubernetes-system-apiserver-alerts:
    AggregatedAPIErrors:
      annotations:
        description: An aggregated API {{  $labels.name  }}/{{  $labels.namespace  }} has reported errors. The number of errors have increased for it in the past five minutes. High values indicate that the availability of the service changes too often.
        summary: An aggregated API has reported errors.
      expr: sum by(name, namespace)(increase(aggregator_unavailable_apiservice_count[5m])) > 2
      labels:
        severity: warning
        controlPlane: true
    AggregatedAPIDown:
      annotations:
        description: An aggregated API {{  $labels.name  }}/{{  $labels.namespace  }} has been only {{  $value | humanize  }}% available over the last 10m.
        summary: An aggregated API is down.
      expr: (1 - max by(name, namespace)(avg_over_time(aggregator_unavailable_apiservice[10m]))) * 100 < 85
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    KubeAPIDown:
      annotations:
        description: KubeAPI has disappeared from Prometheus target discovery.
        summary: Target disappeared from Prometheus target discovery.
      expr: absent(up{job="apiserver"} == 1)
      for: 15m
      labels:
        severity: critical
        controlPlane: true
  kubernetes-system-kubelet-alerts:
    KubeNodeNotReady:
      annotations:
        description: '{{  $labels.node  }} has been unready for more than 15 minutes.'
        summary: Node is not ready.
      expr: kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    KubeNodeUnreachable:
      annotations:
        description: '{{  $labels.node  }} is unreachable and some workloads may be rescheduled.'
        summary: Node is unreachable.
      expr: (kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"} unless ignoring(key,value) kube_node_spec_taint{job="kube-state-metrics",key=~"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn"}) == 1
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    KubeletTooManyPods:
      annotations:
        description: Kubelet '{{  $labels.node  }}' is running at {{  $value | humanizePercentage  }} of its Pod capacity.
        summary: Kubelet is running at capacity.
      expr: |-
        count by(node) (
          (kube_pod_status_phase{job="kube-state-metrics",phase="Running"} == 1) * on(instance,pod,namespace,cluster) group_left(node) topk by(instance,pod,namespace,cluster) (1, kube_pod_info{job="kube-state-metrics"})
        )
        /
        max by(node) (
          kube_node_status_capacity_pods{job="kube-state-metrics"} != 1
        ) > 0.95
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    KubeNodeReadinessFlapping:
      annotations:
        description: The readiness status of node {{  $labels.node  }} has changed {{  $value  }} times in the last 15 minutes.
        summary: Node readiness status is flapping.
      expr: sum(changes(kube_node_status_condition{status="true",condition="Ready"}[15m])) by (node) > 2
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    KubeletPlegDurationHigh:
      annotations:
        description: The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of {{  $value  }} seconds on node {{  $labels.node  }}.
        summary: Kubelet Pod Lifecycle Event Generator is taking too long to relist.
      expr: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile="0.99"} >= 10
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    KubeletPodStartUpLatencyHigh:
      annotations:
        description: Kubelet Pod startup 99th percentile latency is {{  $value  }} seconds on node {{  $labels.node  }}.
        summary: Kubelet Pod startup latency is too high.
      expr: histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (instance, le)) * on(instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"} > 60
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    KubeletDown:
      annotations:
        description: Kubelet has disappeared from Prometheus target discovery.
        summary: Target disappeared from Prometheus target discovery.
      expr: absent(up{job="kubelet", metrics_path="/metrics"} == 1)
      for: 15m
      labels:
        severity: critical
        controlPlane: true
    KubeletRuntimeErrors:
      annotations:
        description: Kubelet has errors.
        summary: Kubelet has errors.
      expr: increase(kubelet_runtime_operations_errors_total[1m]) > 0
      for: 5m
      labels:
        severity: critical
        controlPlane: true
  kubernetes-system-alerts:
    KubeVersionMismatch:
      annotations:
        description: There are {{  $value  }} different semantic versions of Kubernetes components running.
        summary: Different semantic versions of Kubernetes components running.
      expr: count(count by (gitVersion) (label_replace(kubernetes_build_info{job!~"kube-dns|coredns"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*).*"))) > 1
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    KubeClientErrors:
      annotations:
        description: Kubernetes API server client '{{  $labels.job  }}/{{  $labels.instance  }}' is experiencing {{  $value | humanizePercentage  }} errors.'
        summary: Kubernetes API server client is experiencing errors.
      expr: |-
        (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (instance, job)
          /
        sum(rate(rest_client_requests_total[5m])) by (instance, job))
        > 0.01
      for: 15m
      labels:
        severity: warning
        controlPlane: true
  node-exporter-alerts:
    NodeFilesystemSpaceFillingUp24H:
      annotations:
        description: Filesystem on {{  $labels.device  }} at {{  $labels.instance  }} has only {{  printf "%.2f" $value  }}% available space left and is filling up.
        summary: Filesystem is predicted to run out of space within the next 24 hours.
      expr: |-
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 40
        and
          predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!=""}[6h], 24*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
        controlPlane: true
    NodeFilesystemSpaceFillingUp4H:
      annotations:
        description: Filesystem on {{  $labels.device  }} at {{  $labels.instance  }} has only {{  printf "%.2f" $value  }}% available space left and is filling up fast.
        summary: Filesystem is predicted to run out of space within the next 4 hours.
      expr: |-
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 15
        and
          predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!=""}[6h], 4*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: critical
        controlPlane: true
    NodeFilesystemAlmostOutOfSpace5%:
      annotations:
        description: Filesystem on {{  $labels.device  }} at {{  $labels.instance  }} has only {{  printf "%.2f" $value  }}% available space left.
        summary: Filesystem has less than 5% space left.
      expr: |-
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 5
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
        controlPlane: true
    NodeFilesystemAlmostOutOfSpace3%:
      annotations:
        description: Filesystem on {{  $labels.device  }} at {{  $labels.instance  }} has only {{  printf "%.2f" $value  }}% available space left.
        summary: Filesystem has less than 3% space left.
      expr: |-
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 3
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: critical
        controlPlane: true
    NodeFilesystemFilesFillingUp24H:
      annotations:
        description: Filesystem on {{  $labels.device  }} at {{  $labels.instance  }} has only {{  printf "%.2f" $value  }}% available inodes left and is filling up.
        summary: Filesystem is predicted to run out of inodes within the next 24 hours.
      expr: |-
        (
          node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 40
        and
          predict_linear(node_filesystem_files_free{job="node-exporter",fstype!=""}[6h], 24*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
        controlPlane: true
    NodeFilesystemFilesFillingUp4H:
      annotations:
        description: Filesystem on {{  $labels.device  }} at {{  $labels.instance  }} has only {{  printf "%.2f" $value  }}% available inodes left and is filling up fast.
        summary: Filesystem is predicted to run out of inodes within the next 4 hours.
      expr: |-
        (
          node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 20
        and
          predict_linear(node_filesystem_files_free{job="node-exporter",fstype!=""}[6h], 4*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: critical
        controlPlane: true
    NodeFilesystemAlmostOutOfFiles5%:
      annotations:
        description: Filesystem on {{  $labels.device  }} at {{  $labels.instance  }} has only {{  printf "%.2f" $value  }}% available inodes left.
        summary: Filesystem has less than 5% inodes left.
      expr: |-
        (
          node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 5
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
        controlPlane: true
    NodeFilesystemAlmostOutOfFiles3%:
      annotations:
        description: Filesystem on {{  $labels.device  }} at {{  $labels.instance  }} has only {{  printf "%.2f" $value  }}% available inodes left.
        summary: Filesystem has less than 3% inodes left.
      expr: |-
        (
          node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 3
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: critical
        controlPlane: true
    NodeNetworkReceiveErrs:
      annotations:
        description: '{{  $labels.instance  }} interface {{  $labels.device  }} has encountered {{  printf "%.0f" $value  }} receive errors in the last two minutes.'
        summary: Network interface is reporting many receive errors.
      expr: rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01
      for: 1h
      labels:
        severity: warning
        controlPlane: true
    NodeNetworkTransmitErrs:
      annotations:
        description: '{{  $labels.instance  }} interface {{  $labels.device  }} has encountered {{  printf "%.0f" $value  }} transmit errors in the last two minutes.'
        summary: Network interface is reporting many transmit errors.
      expr: rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01
      for: 1h
      labels:
        severity: warning
        controlPlane: true
    NodeHighNumberConntrackEntriesUsed:
      annotations:
        description: '{{  $value | humanizePercentage  }} of conntrack entries are used.'
        summary: Number of conntrack are getting close to the limit.
      expr: (node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75
      labels:
        severity: warning
        controlPlane: true
    NodeTextFileCollectorScrapeError:
      annotations:
        description: Node Exporter text file collector failed to scrape.
        summary: Node Exporter text file collector failed to scrape.
      expr: node_textfile_scrape_error{job="node-exporter"} == 1
      labels:
        severity: warning
        controlPlane: true
    NodeClockSkewDetected:
      annotations:
        message: Clock on {{  $labels.instance  }} is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
        summary: Clock skew detected.
      expr: |-
        (
          node_timex_offset_seconds > 0.05
        and
          deriv(node_timex_offset_seconds[5m]) >= 0
        )
        or
        (
          node_timex_offset_seconds < -0.05
        and
          deriv(node_timex_offset_seconds[5m]) <= 0
        )
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    NodeClockNotSynchronising:
      annotations:
        message: Clock on {{  $labels.instance  }} is not synchronising. Ensure NTP is configured on this host.
        summary: Clock not synchronising.
      expr: |-
        min_over_time(node_timex_sync_status[5m]) == 0
        and
        node_timex_maxerror_seconds >= 16
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    NodeRAIDDegraded:
      annotations:
        description: RAID array '{{  $labels.device  }}' on {{  $labels.instance  }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
        summary: RAID Array is degraded
      expr: node_md_disks_required - ignoring (state) (node_md_disks{state="active"}) > 0
      for: 15m
      labels:
        severity: critical
        controlPlane: true
    NodeRAIDDiskFailure:
      annotations:
        description: At least one device in RAID array on {{  $labels.instance  }} failed. Array '{{  $labels.device  }}' needs attention and possibly a disk swap.
        summary: Failed device in RAID array
      expr: node_md_disks{state="fail"} > 0
      labels:
        severity: warning
        controlPlane: true
  node-network-alerts:
    NodeNetworkInterfaceFlapping:
      annotations:
        description: Network interface "{{  $labels.device  }}" changing it's up status often on node-exporter {{  $labels.namespace  }}/{{  $labels.pod  }}"
      expr: changes(node_network_up{job="node-exporter",device!~"veth.+"}[2m]) > 2
      for: 2m
      labels:
        severity: warning
        controlPlane: true
  aws-auto-scaling:
    AwsAutoScalingGroupMaxedOutCritical:
      expr: (cloudwatch_aws_auto_scaling_group_total_instances_average/cloudwatch_aws_auto_scaling_group_max_size_average) * 100 == 100
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: AWS Auto-Scaling Group reached the maximum available nodes
        summary: AWS Auto-Scaling Group maxed out
    AwsAutoScalingGroupMaxedOutWarning:
      expr: (cloudwatch_aws_auto_scaling_group_total_instances_average/cloudwatch_aws_auto_scaling_group_max_size_average) * 100 > 85
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        description: AWS Auto-Scaling Group is 85% utilized.
        summary: AWS Auto-Scaling Group getting close to the maximum
    AwsAutoScalingGroupCantScaleUp:
      expr: (cloudwatch_aws_auto_scaling_group_desired_capacity_average - cloudwatch_aws_auto_scaling_group_total_instances_average) > 0
      for: 10m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        description: AWS Auto-Scaling Group can't scale up and has not reached the maximum capacity
        summary: AWS Auto-Scaling Group can't scale up
